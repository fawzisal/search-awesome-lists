





<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
  <link rel="dns-prefetch" href="https://assets-cdn.github.com">
  <link rel="dns-prefetch" href="https://avatars0.githubusercontent.com">
  <link rel="dns-prefetch" href="https://avatars1.githubusercontent.com">
  <link rel="dns-prefetch" href="https://avatars2.githubusercontent.com">
  <link rel="dns-prefetch" href="https://avatars3.githubusercontent.com">
  <link rel="dns-prefetch" href="https://github-cloud.s3.amazonaws.com">
  <link rel="dns-prefetch" href="https://user-images.githubusercontent.com/">



  <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://assets-cdn.github.com/assets/frameworks-592c4aa40e940d1b0607a3cf272916ff.css" />
  <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://assets-cdn.github.com/assets/github-96ebb1551fc5dba84c6d2a0fa7b1cfcf.css" />
  
  
  <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://assets-cdn.github.com/assets/site-348211d27070b0d7bb5d31b1ac3d265b.css" />
  

  <meta name="viewport" content="width=device-width">
  
  <title>GitHub - terryum/awesome-deep-learning-papers: The most cited deep learning papers</title>
    <meta name="description" content="GitHub is where people build software. More than 27 million people use GitHub to discover, fork, and contribute to over 80 million projects.">
  <link rel="search" type="application/opensearchdescription+xml" href="/opensearch.xml" title="GitHub">
  <link rel="fluid-icon" href="https://github.com/fluidicon.png" title="GitHub">
  <meta property="fb:app_id" content="1401488693436528">

    
    <meta property="og:image" content="https://avatars0.githubusercontent.com/u/12528769?s=400&amp;v=4" /><meta property="og:site_name" content="GitHub" /><meta property="og:type" content="object" /><meta property="og:title" content="terryum/awesome-deep-learning-papers" /><meta property="og:url" content="https://github.com/terryum/awesome-deep-learning-papers" /><meta property="og:description" content="awesome-deep-learning-papers - The most cited deep learning papers" />

  <link rel="assets" href="https://assets-cdn.github.com/">
  
  <meta name="pjax-timeout" content="1000">
  
  <meta name="request-id" content="E0A0:5D4E:264E036:470E569:5AD2F69B" data-pjax-transient>


  

  <meta name="selected-link" value="repo_source" data-pjax-transient>

    <meta name="google-site-verification" content="KT5gs8h0wvaagLKAVWq8bbeNwnZZK1r1XQysX3xurLU">
  <meta name="google-site-verification" content="ZzhVyEFwb7w3e0-uOTltm8Jsck2F5StVihD0exw2fsA">
  <meta name="google-site-verification" content="GXs5KoUUkNCoaAZn7wPN-t01Pywp9M3sEjnt_3_ZWPc">
    <meta name="google-analytics" content="UA-3769691-2">

<meta name="octolytics-host" content="collector.githubapp.com" /><meta name="octolytics-app-id" content="github" /><meta name="octolytics-event-url" content="https://collector.githubapp.com/github-external/browser_event" /><meta name="octolytics-dimension-request_id" content="E0A0:5D4E:264E036:470E569:5AD2F69B" /><meta name="octolytics-dimension-region_edge" content="iad" /><meta name="octolytics-dimension-region_render" content="iad" />
<meta name="analytics-location" content="/&lt;user-name&gt;/&lt;repo-name&gt;" data-pjax-transient="true" />




  <meta class="js-ga-set" name="dimension1" content="Logged Out">


  

      <meta name="hostname" content="github.com">
    <meta name="user-login" content="">

      <meta name="expected-hostname" content="github.com">
    <meta name="js-proxy-site-detection-payload" content="ODFlNWYxOTJiYTQ4OGNkYWE5ZmUwMjhiNzMwYWM1MzVlOGNjNzhjMjliZTMzNGZhMGE0ZGQ2NzAyZWY1OGNiOXx7InJlbW90ZV9hZGRyZXNzIjoiMTA4LjE2OC4zNS4xOTkiLCJyZXF1ZXN0X2lkIjoiRTBBMDo1RDRFOjI2NEUwMzY6NDcwRTU2OTo1QUQyRjY5QiIsInRpbWVzdGFtcCI6MTUyMzc3NTEzMiwiaG9zdCI6ImdpdGh1Yi5jb20ifQ==">

    <meta name="enabled-features" content="UNIVERSE_BANNER,FREE_TRIALS,MARKETPLACE_INSIGHTS,MARKETPLACE_SELF_SERVE,MARKETPLACE_INSIGHTS_CONVERSION_PERCENTAGES">

  <meta name="html-safe-nonce" content="529ca443e5b51eb5aa11f550d9da70a9eb99ebf7">

  <meta http-equiv="x-pjax-version" content="402da7acb22ff2e9e7f63736b021d40d">
  

      <link href="https://github.com/terryum/awesome-deep-learning-papers/commits/master.atom" rel="alternate" title="Recent Commits to awesome-deep-learning-papers:master" type="application/atom+xml">

  <meta name="description" content="awesome-deep-learning-papers - The most cited deep learning papers">
  <meta name="go-import" content="github.com/terryum/awesome-deep-learning-papers git https://github.com/terryum/awesome-deep-learning-papers.git">

  <meta name="octolytics-dimension-user_id" content="12528769" /><meta name="octolytics-dimension-user_login" content="terryum" /><meta name="octolytics-dimension-repository_id" content="60325062" /><meta name="octolytics-dimension-repository_nwo" content="terryum/awesome-deep-learning-papers" /><meta name="octolytics-dimension-repository_public" content="true" /><meta name="octolytics-dimension-repository_is_fork" content="false" /><meta name="octolytics-dimension-repository_network_root_id" content="60325062" /><meta name="octolytics-dimension-repository_network_root_nwo" content="terryum/awesome-deep-learning-papers" /><meta name="octolytics-dimension-repository_explore_github_marketplace_ci_cta_shown" content="false" />


    <link rel="canonical" href="https://github.com/terryum/awesome-deep-learning-papers" data-pjax-transient>


  <meta name="browser-stats-url" content="https://api.github.com/_private/browser/stats">

  <meta name="browser-errors-url" content="https://api.github.com/_private/browser/errors">

  <link rel="mask-icon" href="https://assets-cdn.github.com/pinned-octocat.svg" color="#000000">
  <link rel="icon" type="image/x-icon" class="js-site-favicon" href="https://assets-cdn.github.com/favicon.ico">

<meta name="theme-color" content="#1e2327">



<link rel="manifest" href="/manifest.json" crossOrigin="use-credentials">

  </head>

  <body class="logged-out env-production">
    

  <div class="position-relative js-header-wrapper ">
    <a href="#start-of-content" tabindex="1" class="px-2 py-4 bg-blue text-white show-on-focus js-skip-to-content">Skip to content</a>
    <div id="js-pjax-loader-bar" class="pjax-loader-bar"><div class="progress"></div></div>

    
    
    



        <header class="Header header-logged-out  position-relative f4 py-3" role="banner">
  <div class="container-lg d-flex px-3">
    <div class="d-flex flex-justify-between flex-items-center">
      <a class="header-logo-invertocat my-0" href="https://github.com/" aria-label="Homepage" data-ga-click="(Logged out) Header, go to homepage, icon:logo-wordmark">
        <svg height="32" class="octicon octicon-mark-github" viewBox="0 0 16 16" version="1.1" width="32" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"/></svg>
      </a>

    </div>

    <div class="HeaderMenu HeaderMenu--bright d-flex flex-justify-between flex-auto">
        <nav class="mt-0">
          <ul class="d-flex list-style-none">
              <li class="ml-2">
                <a class="js-selected-navigation-item HeaderNavlink px-0 py-2 m-0" data-ga-click="Header, click, Nav menu - item:features" data-selected-links="/features /features/project-management /features/code-review /features/project-management /features/integrations /features" href="/features">
                  Features
</a>              </li>
              <li class="ml-4">
                <a class="js-selected-navigation-item HeaderNavlink px-0 py-2 m-0" data-ga-click="Header, click, Nav menu - item:business" data-selected-links="/business /business/security /business/customers /business" href="/business">
                  Business
</a>              </li>

              <li class="ml-4">
                <a class="js-selected-navigation-item HeaderNavlink px-0 py-2 m-0" data-ga-click="Header, click, Nav menu - item:explore" data-selected-links="/explore /trending /trending/developers /integrations /integrations/feature/code /integrations/feature/collaborate /integrations/feature/ship showcases showcases_search showcases_landing /explore" href="/explore">
                  Explore
</a>              </li>

              <li class="ml-4">
                    <a class="js-selected-navigation-item HeaderNavlink px-0 py-2 m-0" data-ga-click="Header, click, Nav menu - item:marketplace" data-selected-links=" /marketplace" href="/marketplace">
                      Marketplace
</a>              </li>
              <li class="ml-4">
                <a class="js-selected-navigation-item HeaderNavlink px-0 py-2 m-0" data-ga-click="Header, click, Nav menu - item:pricing" data-selected-links="/pricing /pricing/developer /pricing/team /pricing/business-hosted /pricing/business-enterprise /pricing" href="/pricing">
                  Pricing
</a>              </li>
          </ul>
        </nav>

      <div class="d-flex">
          <div class="d-lg-flex flex-items-center mr-3">
            <div class="header-search scoped-search site-scoped-search js-site-search" role="search">
  <!-- '"` --><!-- </textarea></xmp> --></option></form><form class="js-site-search-form" data-scoped-search-url="/terryum/awesome-deep-learning-papers/search" data-unscoped-search-url="/search" action="/terryum/awesome-deep-learning-papers/search" accept-charset="UTF-8" method="get"><input name="utf8" type="hidden" value="&#x2713;" />
    <label class="form-control header-search-wrapper  js-chromeless-input-container">
          <a class="header-search-scope no-underline" href="/terryum/awesome-deep-learning-papers">This repository</a>
      <input type="text"
        class="form-control header-search-input  js-site-search-focus js-site-search-field is-clearable"
        data-hotkey="s,/"
        name="q"
        value=""
        placeholder="Search"
        aria-label="Search this repository"
        data-unscoped-placeholder="Search GitHub"
        data-scoped-placeholder="Search"
        autocapitalize="off"
        >
        <input type="hidden" class="js-site-search-type-field" name="type" >
    </label>
</form></div>

          </div>

        <span class="d-inline-block">
            <div class="HeaderNavlink px-0 py-2 m-0">
              <a class="text-bold text-white no-underline" href="/login?return_to=%2Fterryum%2Fawesome-deep-learning-papers" data-ga-click="(Logged out) Header, clicked Sign in, text:sign-in">Sign in</a>
                <span class="text-gray">or</span>
                <a class="text-bold text-white no-underline" href="/join?source=header-repo" data-ga-click="(Logged out) Header, clicked Sign up, text:sign-up">Sign up</a>
            </div>
        </span>
      </div>
    </div>
  </div>
</header>

  </div>

  <div id="start-of-content" class="show-on-focus"></div>

    <div id="js-flash-container">
</div>



  <div role="main" class="application-main ">
        <div itemscope itemtype="http://schema.org/SoftwareSourceCode" class="">
    <div id="js-repo-pjax-container" data-pjax-container >
      





  <div class="pagehead repohead instapaper_ignore readability-menu experiment-repo-nav  ">
    <div class="repohead-details-container clearfix container">

      <ul class="pagehead-actions">
  <li>
      <a href="/login?return_to=%2Fterryum%2Fawesome-deep-learning-papers"
    class="btn btn-sm btn-with-count tooltipped tooltipped-n"
    aria-label="You must be signed in to watch a repository" rel="nofollow">
    <svg class="octicon octicon-eye" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.06 2C3 2 0 8 0 8s3 6 8.06 6C13 14 16 8 16 8s-3-6-7.94-6zM8 12c-2.2 0-4-1.78-4-4 0-2.2 1.8-4 4-4 2.22 0 4 1.8 4 4 0 2.22-1.78 4-4 4zm2-4c0 1.11-.89 2-2 2-1.11 0-2-.89-2-2 0-1.11.89-2 2-2 1.11 0 2 .89 2 2z"/></svg>
    Watch
  </a>
  <a class="social-count" href="/terryum/awesome-deep-learning-papers/watchers"
     aria-label="1638 users are watching this repository">
    1,638
  </a>

  </li>

  <li>
      <a href="/login?return_to=%2Fterryum%2Fawesome-deep-learning-papers"
    class="btn btn-sm btn-with-count tooltipped tooltipped-n"
    aria-label="You must be signed in to star a repository" rel="nofollow">
    <svg class="octicon octicon-star" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M14 6l-4.9-.64L7 1 4.9 5.36 0 6l3.6 3.26L2.67 14 7 11.67 11.33 14l-.93-4.74z"/></svg>
    Star
  </a>

    <a class="social-count js-social-count" href="/terryum/awesome-deep-learning-papers/stargazers"
      aria-label="14396 users starred this repository">
      14,396
    </a>

  </li>

  <li>
      <a href="/login?return_to=%2Fterryum%2Fawesome-deep-learning-papers"
        class="btn btn-sm btn-with-count tooltipped tooltipped-n"
        aria-label="You must be signed in to fork a repository" rel="nofollow">
        <svg class="octicon octicon-repo-forked" viewBox="0 0 10 16" version="1.1" width="10" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1a1.993 1.993 0 0 0-1 3.72V6L5 8 3 6V4.72A1.993 1.993 0 0 0 2 1a1.993 1.993 0 0 0-1 3.72V6.5l3 3v1.78A1.993 1.993 0 0 0 5 15a1.993 1.993 0 0 0 1-3.72V9.5l3-3V4.72A1.993 1.993 0 0 0 8 1zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zm3 10c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zm3-10c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z"/></svg>
        Fork
      </a>

    <a href="/terryum/awesome-deep-learning-papers/network" class="social-count"
       aria-label="2588 users forked this repository">
      2,588
    </a>
  </li>
</ul>

      <h1 class="public ">
  <svg class="octicon octicon-repo" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9H3V8h1v1zm0-3H3v1h1V6zm0-2H3v1h1V4zm0-2H3v1h1V2zm8-1v12c0 .55-.45 1-1 1H6v2l-1.5-1.5L3 16v-2H1c-.55 0-1-.45-1-1V1c0-.55.45-1 1-1h10c.55 0 1 .45 1 1zm-1 10H1v2h2v-1h3v1h5v-2zm0-10H2v9h9V1z"/></svg>
  <span class="author" itemprop="author"><a class="url fn" rel="author" href="/terryum">terryum</a></span><!--
--><span class="path-divider">/</span><!--
--><strong itemprop="name"><a data-pjax="#js-repo-pjax-container" href="/terryum/awesome-deep-learning-papers">awesome-deep-learning-papers</a></strong>

</h1>

    </div>
    
<nav class="reponav js-repo-nav js-sidenav-container-pjax container"
     itemscope
     itemtype="http://schema.org/BreadcrumbList"
     role="navigation"
     data-pjax="#js-repo-pjax-container">

  <span itemscope itemtype="http://schema.org/ListItem" itemprop="itemListElement">
    <a class="js-selected-navigation-item selected reponav-item" itemprop="url" data-hotkey="g c" data-selected-links="repo_source repo_downloads repo_commits repo_releases repo_tags repo_branches repo_packages /terryum/awesome-deep-learning-papers" href="/terryum/awesome-deep-learning-papers">
      <svg class="octicon octicon-code" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M9.5 3L8 4.5 11.5 8 8 11.5 9.5 13 14 8 9.5 3zm-5 0L0 8l4.5 5L6 11.5 2.5 8 6 4.5 4.5 3z"/></svg>
      <span itemprop="name">Code</span>
      <meta itemprop="position" content="1">
</a>  </span>

    <span itemscope itemtype="http://schema.org/ListItem" itemprop="itemListElement">
      <a itemprop="url" data-hotkey="g i" class="js-selected-navigation-item reponav-item" data-selected-links="repo_issues repo_labels repo_milestones /terryum/awesome-deep-learning-papers/issues" href="/terryum/awesome-deep-learning-papers/issues">
        <svg class="octicon octicon-issue-opened" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"/></svg>
        <span itemprop="name">Issues</span>
        <span class="Counter">8</span>
        <meta itemprop="position" content="2">
</a>    </span>

  <span itemscope itemtype="http://schema.org/ListItem" itemprop="itemListElement">
    <a data-hotkey="g p" itemprop="url" class="js-selected-navigation-item reponav-item" data-selected-links="repo_pulls checks /terryum/awesome-deep-learning-papers/pulls" href="/terryum/awesome-deep-learning-papers/pulls">
      <svg class="octicon octicon-git-pull-request" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z"/></svg>
      <span itemprop="name">Pull requests</span>
      <span class="Counter">10</span>
      <meta itemprop="position" content="3">
</a>  </span>

    <a data-hotkey="g b" class="js-selected-navigation-item reponav-item" data-selected-links="repo_projects new_repo_project repo_project /terryum/awesome-deep-learning-papers/projects" href="/terryum/awesome-deep-learning-papers/projects">
      <svg class="octicon octicon-project" viewBox="0 0 15 16" version="1.1" width="15" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M10 12h3V2h-3v10zm-4-2h3V2H6v8zm-4 4h3V2H2v12zm-1 1h13V1H1v14zM14 0H1a1 1 0 0 0-1 1v14a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1V1a1 1 0 0 0-1-1z"/></svg>
      Projects
      <span class="Counter" >0</span>
</a>


  <a class="js-selected-navigation-item reponav-item" data-selected-links="repo_graphs repo_contributors dependency_graph pulse /terryum/awesome-deep-learning-papers/pulse" href="/terryum/awesome-deep-learning-papers/pulse">
    <svg class="octicon octicon-graph" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M16 14v1H0V0h1v14h15zM5 13H3V8h2v5zm4 0H7V3h2v10zm4 0h-2V6h2v7z"/></svg>
    Insights
</a>

</nav>


  </div>

<div class="container new-discussion-timeline experiment-repo-nav  ">
  <div class="repository-content ">

    
      <div class="signup-prompt-bg rounded-1">
      <div class="signup-prompt p-4 text-center mb-4 rounded-1">
        <div class="position-relative">
          <!-- '"` --><!-- </textarea></xmp> --></option></form><form action="/site/dismiss_signup_prompt" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="OMZ1/D7IPZ6SjFhsyOcnRdQ0+VXwX6b61Z+G6yzvTtOHI9scSx+WiM80srjpoCy9F6v2FhxT5ueLhez5fUAbVw==" />
            <button type="submit" class="position-absolute top-0 right-0 btn-link link-gray" data-ga-click="(Logged out) Sign up prompt, clicked Dismiss, text:dismiss">
              Dismiss
            </button>
</form>
          <h3 class="pt-2">Join GitHub today</h3>
          <p class="col-6 mx-auto">GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together.</p>
          <p class="pb-2">
            <a class="btn btn-blue" href="/join?source=prompt-code" data-ga-click="(Logged out) Sign up prompt, clicked Sign up, text:sign-up">Sign up</a>
          </p>
        </div>
      </div>
    </div>


  <div class="js-repo-meta-container">
  <div class="repository-meta mb-0  js-repo-meta-edit js-details-container ">
    <div class="repository-meta-content col-11 mb-1">
          <span class="col-11 text-gray-dark mr-2" itemprop="about">
            The most cited deep learning papers
          </span>
    </div>

  </div>

    <div class="mb-3 repository-topics-container js-repository-topics-container js-details-container">
      <div id="topics-list-container" data-url="/terryum/awesome-deep-learning-papers/settings/topics">
          <div class="list-topics-container f6 mt-1">
      <a href="/topics/deep-learning" class="topic-tag topic-tag-link" data-ga-click="Topic, repository page" data-octo-click="topic_click" data-octo-dimensions="topic:deep-learning">
        deep-learning
      </a>
      <a href="/topics/deep-neural-networks" class="topic-tag topic-tag-link" data-ga-click="Topic, repository page" data-octo-click="topic_click" data-octo-dimensions="topic:deep-neural-networks">
        deep-neural-networks
      </a>
      <a href="/topics/machine-learning" class="topic-tag topic-tag-link" data-ga-click="Topic, repository page" data-octo-click="topic_click" data-octo-dimensions="topic:machine-learning">
        machine-learning
      </a>
  </div>


      </div>

    </div>
</div>



  <div class="overall-summary overall-summary-bottomless">
    <div class="stats-switcher-viewport js-stats-switcher-viewport">
      <div class="stats-switcher-wrapper">
      <ul class="numbers-summary">
        <li class="commits">
          <a data-pjax href="/terryum/awesome-deep-learning-papers/commits/master">
              <svg class="octicon octicon-history" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 13H6V6h5v2H8v5zM7 1C4.81 1 2.87 2.02 1.59 3.59L0 2v4h4L2.5 4.5C3.55 3.17 5.17 2.3 7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-.34.03-.67.09-1H.08C.03 7.33 0 7.66 0 8c0 3.86 3.14 7 7 7s7-3.14 7-7-3.14-7-7-7z"/></svg>
              <span class="num text-emphasized">
                167
              </span>
              commits
          </a>
        </li>
        <li>
          <a data-pjax href="/terryum/awesome-deep-learning-papers/branches">
            <svg class="octicon octicon-git-branch" viewBox="0 0 10 16" version="1.1" width="10" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M10 5c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v.3c-.02.52-.23.98-.63 1.38-.4.4-.86.61-1.38.63-.83.02-1.48.16-2 .45V4.72a1.993 1.993 0 0 0-1-3.72C.88 1 0 1.89 0 3a2 2 0 0 0 1 1.72v6.56c-.59.35-1 .99-1 1.72 0 1.11.89 2 2 2 1.11 0 2-.89 2-2 0-.53-.2-1-.53-1.36.09-.06.48-.41.59-.47.25-.11.56-.17.94-.17 1.05-.05 1.95-.45 2.75-1.25S8.95 7.77 9 6.73h-.02C9.59 6.37 10 5.73 10 5zM2 1.8c.66 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2C1.35 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2zm0 12.41c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zm6-8c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z"/></svg>
            <span class="num text-emphasized">
              1
            </span>
            branch
          </a>
        </li>

        <li>
          <a href="/terryum/awesome-deep-learning-papers/releases">
            <svg class="octicon octicon-tag" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.73 1.73C7.26 1.26 6.62 1 5.96 1H3.5C2.13 1 1 2.13 1 3.5v2.47c0 .66.27 1.3.73 1.77l6.06 6.06c.39.39 1.02.39 1.41 0l4.59-4.59a.996.996 0 0 0 0-1.41L7.73 1.73zM2.38 7.09c-.31-.3-.47-.7-.47-1.13V3.5c0-.88.72-1.59 1.59-1.59h2.47c.42 0 .83.16 1.13.47l6.14 6.13-4.73 4.73-6.13-6.15zM3.01 3h2v2H3V3h.01z"/></svg>
            <span class="num text-emphasized">
              0
            </span>
            releases
          </a>
        </li>

        <li>
            <a href="/terryum/awesome-deep-learning-papers/graphs/contributors">
  <svg class="octicon octicon-organization" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M16 12.999c0 .439-.45 1-1 1H7.995c-.539 0-.994-.447-.995-.999H1c-.54 0-1-.561-1-1 0-2.634 3-4 3-4s.229-.409 0-1c-.841-.621-1.058-.59-1-3 .058-2.419 1.367-3 2.5-3s2.442.58 2.5 3c.058 2.41-.159 2.379-1 3-.229.59 0 1 0 1s1.549.711 2.42 2.088C9.196 9.369 10 8.999 10 8.999s.229-.409 0-1c-.841-.62-1.058-.59-1-3 .058-2.419 1.367-3 2.5-3s2.437.581 2.495 3c.059 2.41-.158 2.38-1 3-.229.59 0 1 0 1s3.005 1.366 3.005 4"/></svg>
    <span class="num text-emphasized">
      30
    </span>
    contributors
</a>

        </li>
      </ul>

        <div class="repository-lang-stats">
          <ol class="repository-lang-stats-numbers">
            <li>
                <a href="/terryum/awesome-deep-learning-papers/search?l=tex"  data-ga-click="Repository, language stats search click, location:repo overview">
                  <span class="color-block language-color" style="background-color:#3D6117;"></span>
                  <span class="lang">TeX</span>
                  <span class="percent">85.7%</span>
                </a>
            </li>
            <li>
                <a href="/terryum/awesome-deep-learning-papers/search?l=python"  data-ga-click="Repository, language stats search click, location:repo overview">
                  <span class="color-block language-color" style="background-color:#3572A5;"></span>
                  <span class="lang">Python</span>
                  <span class="percent">14.3%</span>
                </a>
            </li>
          </ol>
        </div>
      </div>
    </div>
  </div>

    <div class="repository-lang-stats-graph js-toggle-lang-stats" title="Click for language details" data-ga-click="Repository, language bar stats toggle, location:repo overview">
      <span class="language-color" aria-label="TeX 85.7%" style="width:85.7%; background-color:#3D6117;" itemprop="keywords">TeX</span>
      <span class="language-color" aria-label="Python 14.3%" style="width:14.3%; background-color:#3572A5;" itemprop="keywords">Python</span>
    </div>



  <div class="file-navigation in-mid-page">

    <details class="get-repo-select-menu js-get-repo-select-menu float-right position-relative dropdown-details details-reset">
  <summary class="btn btn-sm btn-primary">
    Clone or download
    <span class="dropdown-caret"></span>
  </summary>
  <div class="position-relative">
    <div class="get-repo-modal dropdown-menu dropdown-menu-sw pb-0 js-toggler-container  js-get-repo-modal">

      <div class="get-repo-modal-options">
          <div class="clone-options https-clone-options">

            <h4 class="mb-1">
              Clone with HTTPS
              <a class="muted-link" href="https://help.github.com/articles/which-remote-url-should-i-use" target="_blank" title="Which remote URL should I use?">
                <svg class="octicon octicon-question" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6 10h2v2H6v-2zm4-3.5C10 8.64 8 9 8 9H6c0-.55.45-1 1-1h.5c.28 0 .5-.22.5-.5v-1c0-.28-.22-.5-.5-.5h-1c-.28 0-.5.22-.5.5V7H4c0-1.5 1.5-3 3-3s3 1 3 2.5zM7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7z"/></svg>
              </a>
            </h4>
            <p class="mb-2 get-repo-decription-text">
              Use Git or checkout with SVN using the web URL.
            </p>

            <div class="input-group">
  <input type="text" class="form-control input-monospace input-sm js-url-field" value="https://github.com/terryum/awesome-deep-learning-papers.git" aria-label="Clone this repository at https://github.com/terryum/awesome-deep-learning-papers.git" readonly>
  <div class="input-group-button">
    <clipboard-copy
        value="https://github.com/terryum/awesome-deep-learning-papers.git"
        aria-label="Copy to clipboard"
        class="btn btn-sm tooltipped tooltipped-s"
        copied-label="Copied!">
      <svg class="octicon octicon-clippy" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M2 13h4v1H2v-1zm5-6H2v1h5V7zm2 3V8l-3 3 3 3v-2h5v-2H9zM4.5 9H2v1h2.5V9zM2 12h2.5v-1H2v1zm9 1h1v2c-.02.28-.11.52-.3.7-.19.18-.42.28-.7.3H1c-.55 0-1-.45-1-1V4c0-.55.45-1 1-1h3c0-1.11.89-2 2-2 1.11 0 2 .89 2 2h3c.55 0 1 .45 1 1v5h-1V6H1v9h10v-2zM2 5h8c0-.55-.45-1-1-1H8c-.55 0-1-.45-1-1s-.45-1-1-1-1 .45-1 1-.45 1-1 1H3c-.55 0-1 .45-1 1z"/></svg>
    </clipboard-copy>
  </div>
</div>

          </div>

        <div class="mt-2">
          
<a href="/terryum/awesome-deep-learning-papers/archive/master.zip"
   class="btn btn-outline get-repo-btn
"
   rel="nofollow"
   data-ga-click="Repository, download zip, location:repo overview">
  Download ZIP
</a>

        </div>
      </div>

      <div class="js-modal-download-mac py-2 px-3 d-none">
        <h4 class="lh-condensed mb-3">Launching GitHub Desktop<span class="animated-ellipsis-container"><span class="animated-ellipsis">...</span></span></h4>
        <p class="text-gray">If nothing happens, <a href="https://desktop.github.com/">download GitHub Desktop</a> and try again.</p>
        <p><button class="btn-link js-get-repo-modal-download-back">Go back</button></p>
      </div>

      <div class="js-modal-download-windows py-2 px-3 d-none">
        <h4 class="lh-condensed mb-3">Launching GitHub Desktop<span class="animated-ellipsis-container"><span class="animated-ellipsis">...</span></span></h4>
        <p class="text-gray">If nothing happens, <a href="https://desktop.github.com/">download GitHub Desktop</a> and try again.</p>
        <p><button class="btn-link js-get-repo-modal-download-back">Go back</button></p>
      </div>

      <div class="js-modal-download-xcode py-2 px-3 d-none">
        <h4 class="lh-condensed mb-3">Launching Xcode<span class="animated-ellipsis-container"><span class="animated-ellipsis">...</span></span></h4>
        <p class="text-gray">If nothing happens, <a href="https://developer.apple.com/xcode/">download Xcode</a> and try again.</p>
        <p><button class="btn-link js-get-repo-modal-download-back">Go back</button></p>
      </div>

      <div class="js-modal-download-visual-studio py-2 px-3 d-none">
        <h4 class="lh-condensed mb-3">Launching Visual Studio<span class="animated-ellipsis-container"><span class="animated-ellipsis">...</span></span></h4>
        <p class="text-gray">If nothing happens, <a href="https://visualstudio.github.com/">download the GitHub extension for Visual Studio</a> and try again.</p>
        <p><button class="btn-link js-get-repo-modal-download-back">Go back</button></p>
      </div>

    </div>
  </div>
</details>


  <div class="BtnGroup float-right">

    <a href="/terryum/awesome-deep-learning-papers/find/master"
      class="btn btn-sm empty-icon float-right BtnGroup-item"
      data-pjax
      data-hotkey="t"
      data-ga-click="Repository, find file, location:repo overview">
      Find file
    </a>
  </div>

  
<div class="select-menu branch-select-menu js-menu-container js-select-menu float-left">
  <button class=" btn btn-sm select-menu-button js-menu-target css-truncate" data-hotkey="w"
    
    type="button" aria-label="Switch branches or tags" aria-expanded="false" aria-haspopup="true">
      <i>Branch:</i>
      <span class="js-select-button css-truncate-target">master</span>
  </button>

  <div class="select-menu-modal-holder js-menu-content js-navigation-container" data-pjax>

    <div class="select-menu-modal">
      <div class="select-menu-header">
        <svg class="octicon octicon-x js-menu-close" role="img" aria-label="Close" viewBox="0 0 12 16" version="1.1" width="12" height="16"><path fill-rule="evenodd" d="M7.48 8l3.75 3.75-1.48 1.48L6 9.48l-3.75 3.75-1.48-1.48L4.52 8 .77 4.25l1.48-1.48L6 6.52l3.75-3.75 1.48 1.48z"/></svg>
        <span class="select-menu-title">Switch branches/tags</span>
      </div>

      <div class="select-menu-filters">
        <div class="select-menu-text-filter">
          <input type="text" aria-label="Filter branches/tags" id="context-commitish-filter-field" class="form-control js-filterable-field js-navigation-enable" placeholder="Filter branches/tags">
        </div>
        <div class="select-menu-tabs">
          <ul>
            <li class="select-menu-tab">
              <a href="#" data-tab-filter="branches" data-filter-placeholder="Filter branches/tags" class="js-select-menu-tab" role="tab">Branches</a>
            </li>
            <li class="select-menu-tab">
              <a href="#" data-tab-filter="tags" data-filter-placeholder="Find a tagâ€¦" class="js-select-menu-tab" role="tab">Tags</a>
            </li>
          </ul>
        </div>
      </div>

      <div class="select-menu-list select-menu-tab-bucket js-select-menu-tab-bucket" data-tab-filter="branches" role="menu">

        <div data-filterable-for="context-commitish-filter-field" data-filterable-type="substring">


            <a class="select-menu-item js-navigation-item js-navigation-open selected"
               href="/terryum/awesome-deep-learning-papers/tree/master"
               data-name="master"
               data-skip-pjax="true"
               rel="nofollow">
              <svg class="octicon octicon-check select-menu-item-icon" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M12 5l-8 8-4-4 1.5-1.5L4 10l6.5-6.5z"/></svg>
              <span class="select-menu-item-text css-truncate-target js-select-menu-filter-text">
                master
              </span>
            </a>
        </div>

          <div class="select-menu-no-results">Nothing to show</div>
      </div>

      <div class="select-menu-list select-menu-tab-bucket js-select-menu-tab-bucket" data-tab-filter="tags">
        <div data-filterable-for="context-commitish-filter-field" data-filterable-type="substring">


        </div>

        <div class="select-menu-no-results">Nothing to show</div>
      </div>

    </div>
  </div>
</div>


        <button type="button" class="btn btn-sm disabled tooltipped tooltipped-n new-pull-request-btn" aria-label="You must be signed in to create a pull request">
          New pull request
        </button>

  <div class="breadcrumb">
    
  </div>
</div>


  


  <div class="commit-tease js-details-container Details">
    <span class="float-right">
      Latest commit
      <a class="commit-tease-sha" href="/terryum/awesome-deep-learning-papers/commit/7bc2b16dc7df907ac8aa108d138b8f45d45d38d1" data-pjax>
        7bc2b16
      </a>
      <span itemprop="dateModified"><relative-time datetime="2017-10-03T08:59:59Z">Oct 3, 2017</relative-time></span>
    </span>


    <div class="d-flex no-wrap">
      
<div class="AvatarStack flex-self-start ">
  <div class="AvatarStack-body tooltipped tooltipped-se tooltipped-align-left-1"
       aria-label="terryum">

        <a href="/terryum" data-skip-pjax="true" class="avatar">
          <img src="https://avatars0.githubusercontent.com/u/12528769?s=40&amp;v=4" width="20" height="20" alt="@terryum">
        </a>
  </div>
</div>

      <div class="flex-auto f6">
        
      <a href="/terryum/awesome-deep-learning-papers/commits?author=terryum"
     class="commit-author tooltipped tooltipped-s user-mention"
     aria-label="View all commits by terryum">terryum</a>


  


          <a href="/terryum/awesome-deep-learning-papers/commit/7bc2b16dc7df907ac8aa108d138b8f45d45d38d1" class="message" data-pjax="true" title="Update ReadingNotes.md">Update ReadingNotes.md</a>

      </div>
    </div>
  </div>



<div class="file-wrap">

  <a class="d-none js-permalink-shortcut" data-hotkey="y" href="/terryum/awesome-deep-learning-papers/tree/7bc2b16dc7df907ac8aa108d138b8f45d45d38d1">Permalink</a>

  <table class="files js-navigation-container js-active-navigation-container" data-pjax>


    <tbody>
      <tr class="warning include-fragment-error">
        <td class="icon"><svg class="octicon octicon-alert" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.865 1.52c-.18-.31-.51-.5-.87-.5s-.69.19-.87.5L.275 13.5c-.18.31-.18.69 0 1 .19.31.52.5.87.5h13.7c.36 0 .69-.19.86-.5.17-.31.18-.69.01-1L8.865 1.52zM8.995 13h-2v-2h2v2zm0-3h-2V6h2v4z"/></svg></td>
        <td class="content" colspan="3">Failed to load latest commit information.</td>
      </tr>

        <tr class="js-navigation-item">
          <td class="icon">
            <svg class="octicon octicon-file" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6 5H2V4h4v1zM2 8h7V7H2v1zm0 2h7V9H2v1zm0 2h7v-1H2v1zm10-7.5V14c0 .55-.45 1-1 1H1c-.55 0-1-.45-1-1V2c0-.55.45-1 1-1h7.5L12 4.5zM11 5L8 2H1v12h10V5z"/></svg>
            <img width="16" height="16" class="spinner" alt="" src="https://assets-cdn.github.com/images/spinners/octocat-spinner-32.gif" />
          </td>
          <td class="content">
            <span class="css-truncate css-truncate-target"><a class="js-navigation-open" title=".gitignore" id="a084b794bc0759e7a6b77810e01874f2-d2d7c06f302840e597fbadee1f95849ddea04bab" href="/terryum/awesome-deep-learning-papers/blob/master/.gitignore">.gitignore</a></span>
          </td>
          <td class="message">
            <span class="css-truncate css-truncate-target">
                  <a data-pjax="true" title="Added .gitignore with papers directory" class="message" href="/terryum/awesome-deep-learning-papers/commit/c7fa34e52908f93db9b743c0bbb2d7a03cb8e824">Added .gitignore with papers directory</a>
            </span>
          </td>
          <td class="age">
            <span class="css-truncate css-truncate-target"><time-ago datetime="2017-02-26T18:15:41Z">Feb 26, 2017</time-ago></span>
          </td>
        </tr>
        <tr class="js-navigation-item">
          <td class="icon">
            <svg class="octicon octicon-file" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6 5H2V4h4v1zM2 8h7V7H2v1zm0 2h7V9H2v1zm0 2h7v-1H2v1zm10-7.5V14c0 .55-.45 1-1 1H1c-.55 0-1-.45-1-1V2c0-.55.45-1 1-1h7.5L12 4.5zM11 5L8 2H1v12h10V5z"/></svg>
            <img width="16" height="16" class="spinner" alt="" src="https://assets-cdn.github.com/images/spinners/octocat-spinner-32.gif" />
          </td>
          <td class="content">
            <span class="css-truncate css-truncate-target"><a class="js-navigation-open" title="Contributing.md" id="f0e09ea60b1f1cc6a00329faaa3f7193-d229e693ee198e4cce94fbdf8b14ac130a0ec716" href="/terryum/awesome-deep-learning-papers/blob/master/Contributing.md">Contributing.md</a></span>
          </td>
          <td class="message">
            <span class="css-truncate css-truncate-target">
                  <a data-pjax="true" title="Fix for some small typos in the markdown files" class="message" href="/terryum/awesome-deep-learning-papers/commit/30ac3c8f2cf50ea2e1739db5fd0703dc6396d79f">Fix for some small typos in the markdown files</a>
            </span>
          </td>
          <td class="age">
            <span class="css-truncate css-truncate-target"><time-ago datetime="2017-02-23T06:36:33Z">Feb 23, 2017</time-ago></span>
          </td>
        </tr>
        <tr class="js-navigation-item">
          <td class="icon">
            <svg class="octicon octicon-file" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6 5H2V4h4v1zM2 8h7V7H2v1zm0 2h7V9H2v1zm0 2h7v-1H2v1zm10-7.5V14c0 .55-.45 1-1 1H1c-.55 0-1-.45-1-1V2c0-.55.45-1 1-1h7.5L12 4.5zM11 5L8 2H1v12h10V5z"/></svg>
            <img width="16" height="16" class="spinner" alt="" src="https://assets-cdn.github.com/images/spinners/octocat-spinner-32.gif" />
          </td>
          <td class="content">
            <span class="css-truncate css-truncate-target"><a class="js-navigation-open" title="README.md" id="04c6e90faac2675aa89e2176d2eec7d8-df114810e9672eecae1a608d64029e7585ff3365" href="/terryum/awesome-deep-learning-papers/blob/master/README.md">README.md</a></span>
          </td>
          <td class="message">
            <span class="css-truncate css-truncate-target">
                  <a data-pjax="true" title="Merge pull request #68 from minsangkim142/patch-1

Added Stanford Question Answering Dataset (SQuAD)" class="message" href="/terryum/awesome-deep-learning-papers/commit/117fd9376b431aefa0da8d1b418be1cd3afad821">Merge pull request</a> <a href="https://github.com/terryum/awesome-deep-learning-papers/pull/68" class="issue-link js-issue-link" data-error-text="Failed to load issue title" data-id="259707688" data-permission-text="Issue title is private" data-url="https://github.com/terryum/awesome-deep-learning-papers/issues/68">#68</a> <a data-pjax="true" title="Merge pull request #68 from minsangkim142/patch-1

Added Stanford Question Answering Dataset (SQuAD)" class="message" href="/terryum/awesome-deep-learning-papers/commit/117fd9376b431aefa0da8d1b418be1cd3afad821">from minsangkim142/patch-1</a>
            </span>
          </td>
          <td class="age">
            <span class="css-truncate css-truncate-target"><time-ago datetime="2017-10-03T06:45:19Z">Oct 3, 2017</time-ago></span>
          </td>
        </tr>
        <tr class="js-navigation-item">
          <td class="icon">
            <svg class="octicon octicon-file" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6 5H2V4h4v1zM2 8h7V7H2v1zm0 2h7V9H2v1zm0 2h7v-1H2v1zm10-7.5V14c0 .55-.45 1-1 1H1c-.55 0-1-.45-1-1V2c0-.55.45-1 1-1h7.5L12 4.5zM11 5L8 2H1v12h10V5z"/></svg>
            <img width="16" height="16" class="spinner" alt="" src="https://assets-cdn.github.com/images/spinners/octocat-spinner-32.gif" />
          </td>
          <td class="content">
            <span class="css-truncate css-truncate-target"><a class="js-navigation-open" title="ReadingNotes.md" id="74362dbe43114babf6fc111cdd6e7ea3-112ccc9f0f6152f31f5fe809f2d391a829b33a65" href="/terryum/awesome-deep-learning-papers/blob/master/ReadingNotes.md">ReadingNotes.md</a></span>
          </td>
          <td class="message">
            <span class="css-truncate css-truncate-target">
                  <a data-pjax="true" title="Update ReadingNotes.md" class="message" href="/terryum/awesome-deep-learning-papers/commit/7bc2b16dc7df907ac8aa108d138b8f45d45d38d1">Update ReadingNotes.md</a>
            </span>
          </td>
          <td class="age">
            <span class="css-truncate css-truncate-target"><time-ago datetime="2017-10-03T08:59:59Z">Oct 3, 2017</time-ago></span>
          </td>
        </tr>
        <tr class="js-navigation-item">
          <td class="icon">
            <svg class="octicon octicon-file" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6 5H2V4h4v1zM2 8h7V7H2v1zm0 2h7V9H2v1zm0 2h7v-1H2v1zm10-7.5V14c0 .55-.45 1-1 1H1c-.55 0-1-.45-1-1V2c0-.55.45-1 1-1h7.5L12 4.5zM11 5L8 2H1v12h10V5z"/></svg>
            <img width="16" height="16" class="spinner" alt="" src="https://assets-cdn.github.com/images/spinners/octocat-spinner-32.gif" />
          </td>
          <td class="content">
            <span class="css-truncate css-truncate-target"><a class="js-navigation-open" title="fetch_papers.py" id="1ffa994c94d5c552c4f365aa8a1abb4b-80f1409ca74e1d0a548a3272d879cc349114b9bc" href="/terryum/awesome-deep-learning-papers/blob/master/fetch_papers.py">fetch_papers.py</a></span>
          </td>
          <td class="message">
            <span class="css-truncate css-truncate-target">
                  <a data-pjax="true" title="use of print function instead of print statement in the except clause" class="message" href="/terryum/awesome-deep-learning-papers/commit/f2446b835e6b2353aea1b82d190e449c968e866c">use of print function instead of print statement in the except clause</a>
            </span>
          </td>
          <td class="age">
            <span class="css-truncate css-truncate-target"><time-ago datetime="2017-07-31T05:25:56Z">Jul 31, 2017</time-ago></span>
          </td>
        </tr>
        <tr class="js-navigation-item">
          <td class="icon">
            <svg class="octicon octicon-file" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6 5H2V4h4v1zM2 8h7V7H2v1zm0 2h7V9H2v1zm0 2h7v-1H2v1zm10-7.5V14c0 .55-.45 1-1 1H1c-.55 0-1-.45-1-1V2c0-.55.45-1 1-1h7.5L12 4.5zM11 5L8 2H1v12h10V5z"/></svg>
            <img width="16" height="16" class="spinner" alt="" src="https://assets-cdn.github.com/images/spinners/octocat-spinner-32.gif" />
          </td>
          <td class="content">
            <span class="css-truncate css-truncate-target"><a class="js-navigation-open" title="get_authors.py" id="05f8f7f056ea54faed0fd957e92c630c-11a3312c8534c6a846a34ba03f0f9525d8ffac26" href="/terryum/awesome-deep-learning-papers/blob/master/get_authors.py">get_authors.py</a></span>
          </td>
          <td class="message">
            <span class="css-truncate css-truncate-target">
                  <a data-pjax="true" title="Add get_authors script to list all authors

Not sure the information use case is you wanted. Let me
know if any comments" class="message" href="/terryum/awesome-deep-learning-papers/commit/db6286ccbafedc7f5469abaa0dd951fe57b38b4d">Add get_authors script to list all authors</a>
            </span>
          </td>
          <td class="age">
            <span class="css-truncate css-truncate-target"><time-ago datetime="2017-03-02T15:03:29Z">Mar 2, 2017</time-ago></span>
          </td>
        </tr>
        <tr class="js-navigation-item">
          <td class="icon">
            <svg class="octicon octicon-file" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6 5H2V4h4v1zM2 8h7V7H2v1zm0 2h7V9H2v1zm0 2h7v-1H2v1zm10-7.5V14c0 .55-.45 1-1 1H1c-.55 0-1-.45-1-1V2c0-.55.45-1 1-1h7.5L12 4.5zM11 5L8 2H1v12h10V5z"/></svg>
            <img width="16" height="16" class="spinner" alt="" src="https://assets-cdn.github.com/images/spinners/octocat-spinner-32.gif" />
          </td>
          <td class="content">
            <span class="css-truncate css-truncate-target"><a class="js-navigation-open" title="top100papers.bib" id="ef83563f85c63c2128f159c9df87778d-7c4ac3276ad15c6569d988e300a186f551e846f9" href="/terryum/awesome-deep-learning-papers/blob/master/top100papers.bib">top100papers.bib</a></span>
          </td>
          <td class="message">
            <span class="css-truncate css-truncate-target">
                  <a data-pjax="true" title="Add &quot;Training very deep networks&quot; to bib" class="message" href="/terryum/awesome-deep-learning-papers/commit/2400bbef05c40cfc52ee8064614a81fd602ffb21">Add "Training very deep networks" to bib</a>
            </span>
          </td>
          <td class="age">
            <span class="css-truncate css-truncate-target"><time-ago datetime="2017-05-08T13:36:22Z">May 8, 2017</time-ago></span>
          </td>
        </tr>
    </tbody>
  </table>

</div>



  <div id="readme" class="readme boxed-group clearfix announce instapaper_body md">
    <h3>
      <svg class="octicon octicon-book" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M3 5h4v1H3V5zm0 3h4V7H3v1zm0 2h4V9H3v1zm11-5h-4v1h4V5zm0 2h-4v1h4V7zm0 2h-4v1h4V9zm2-6v9c0 .55-.45 1-1 1H9.5l-1 1-1-1H2c-.55 0-1-.45-1-1V3c0-.55.45-1 1-1h5.5l1 1 1-1H15c.55 0 1 .45 1 1zm-8 .5L7.5 3H2v9h6V3.5zm7-.5H9.5l-.5.5V12h6V3z"/></svg>
      README.md
    </h3>

      <article class="markdown-body entry-content" itemprop="text"><h1><a href="#awesome---most-cited-deep-learning-papers" aria-hidden="true" class="anchor" id="user-content-awesome---most-cited-deep-learning-papers"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Awesome - Most Cited Deep Learning Papers</h1>
<p><a href="https://github.com/sindresorhus/awesome"><img src="https://camo.githubusercontent.com/13c4e50d88df7178ae1882a203ed57b641674f94/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f643733303566333864323966656437386661383536353265336136336531353464643865383832392f6d656469612f62616467652e737667" alt="Awesome" data-canonical-src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" style="max-width:100%;"></a></p>
<p>A curated list of the most cited deep learning papers (since 2012)</p>
<p>We believe that there exist <em>classic</em> deep learning papers which are worth reading regardless of their application domain. Rather than providing overwhelming amount of papers, We would like to provide a <em>curated list</em> of the awesome deep learning papers which are considered as <em>must-reads</em> in certain research domains.</p>
<h2><a href="#background" aria-hidden="true" class="anchor" id="user-content-background"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Background</h2>
<p>Before this list, there exist other <em>awesome deep learning lists</em>, for example, <a href="https://github.com/kjw0612/awesome-deep-vision">Deep Vision</a> and <a href="https://github.com/kjw0612/awesome-rnn">Awesome Recurrent Neural Networks</a>. Also, after this list comes out, another awesome list for deep learning beginners, called <a href="https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap">Deep Learning Papers Reading Roadmap</a>, has been created and loved by many deep learning researchers.</p>
<p>Although the <em>Roadmap List</em> includes lots of important deep learning papers, it feels overwhelming for me to read them all. As I mentioned in the introduction, I believe that seminal works can give us lessons regardless of their application domain. Thus, I would like to introduce <strong>top 100 deep learning papers</strong> here as a good starting point of overviewing deep learning researches.</p>
<p>To get the news for newly released papers everyday, follow my <a href="https://twitter.com/TerryUm_ML" rel="nofollow">twitter</a> or <a href="https://www.facebook.com/terryum.io/" rel="nofollow">facebook page</a>!</p>
<h2><a href="#awesome-list-criteria" aria-hidden="true" class="anchor" id="user-content-awesome-list-criteria"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Awesome list criteria</h2>
<ol>
<li>A list of <strong>top 100 deep learning papers</strong> published from 2012 to 2016 is suggested.</li>
<li>If a paper is added to the list, another paper (usually from *More Papers from 2016" section) should be removed to keep top 100 papers. (Thus, removing papers is also important contributions as well as adding papers)</li>
<li>Papers that are important, but failed to be included in the list, will be listed in <em>More than Top 100</em> section.</li>
<li>Please refer to <em>New Papers</em> and <em>Old Papers</em> sections for the papers published in recent 6 months or before 2012.</li>
</ol>
<p><em>(Citation criteria)</em></p>
<ul>
<li><strong>&lt; 6 months</strong> : <em>New Papers</em> (by discussion)</li>
<li><strong>2016</strong> :  +60 citations or "More Papers from 2016"</li>
<li><strong>2015</strong> :  +200 citations</li>
<li><strong>2014</strong> :  +400 citations</li>
<li><strong>2013</strong> :  +600 citations</li>
<li><strong>2012</strong> :  +800 citations</li>
<li><strong>~2012</strong> : <em>Old Papers</em> (by discussion)</li>
</ul>
<p>Please note that we prefer seminal deep learning papers that can be applied to various researches rather than application papers. For that reason, some papers that meet the criteria may not be accepted while others can be. It depends on the impact of the paper, applicability to other researches scarcity of the research domain, and so on.</p>
<p><strong>We need your contributions!</strong></p>
<p>If you have any suggestions (missing papers, new papers, key researchers or typos), please feel free to edit and pull a request.
(Please read the <a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/Contributing.md">contributing guide</a> for further instructions, though just letting me know the title of papers can also be a big contribution to us.)</p>
<p>(Update) You can download all top-100 papers with <a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/fetch_papers.py">this</a> and collect all authors' names with <a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/get_authors.py">this</a>. Also, <a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/top100papers.bib">bib file</a> for all top-100 papers are available. Thanks, doodhwala, <a href="https://github.com/sunshinemyson">Sven</a> and <a href="https://github.com/grepinsight">grepinsight</a>!</p>
<ul>
<li>Can anyone contribute the code for obtaining the statistics of the authors of Top-100 papers?</li>
</ul>
<h2><a href="#contents" aria-hidden="true" class="anchor" id="user-content-contents"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Contents</h2>
<ul>
<li><a href="#understanding--generalization--transfer">Understanding / Generalization / Transfer</a></li>
<li><a href="#optimization--training-techniques">Optimization / Training Techniques</a></li>
<li><a href="#unsupervised--generative-models">Unsupervised / Generative Models</a></li>
<li><a href="#convolutional-neural-network-models">Convolutional Network Models</a></li>
<li><a href="#image-segmentation--object-detection">Image Segmentation / Object Detection</a></li>
<li><a href="#image--video--etc">Image / Video / Etc</a></li>
<li><a href="#natural-language-processing--rnns">Natural Language Processing / RNNs</a></li>
<li><a href="#speech--other-domain">Speech / Other Domain</a></li>
<li><a href="#reinforcement-learning--robotics">Reinforcement Learning / Robotics</a></li>
<li><a href="#more-papers-from-2016">More Papers from 2016</a></li>
</ul>
<p><em>(More than Top 100)</em></p>
<ul>
<li><a href="#new-papers">New Papers</a> : Less than 6 months</li>
<li><a href="#old-papers">Old Papers</a> : Before 2012</li>
<li><a href="#hw--sw--dataset">HW / SW / Dataset</a> : Technical reports</li>
<li><a href="#book--survey--review">Book / Survey / Review</a></li>
<li><a href="#video-lectures--tutorials--blogs">Video Lectures / Tutorials / Blogs</a></li>
<li><a href="#appendix-more-than-top-100">Appendix: More than Top 100</a> : More papers not in the list</li>
</ul>
<hr>
<h3><a href="#understanding--generalization--transfer" aria-hidden="true" class="anchor" id="user-content-understanding--generalization--transfer"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Understanding / Generalization / Transfer</h3>
<ul>
<li><strong>Distilling the knowledge in a neural network</strong> (2015), G. Hinton et al. <a href="http://arxiv.org/pdf/1503.02531" rel="nofollow">[pdf]</a></li>
<li><strong>Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</strong> (2015), A. Nguyen et al. <a href="http://arxiv.org/pdf/1412.1897" rel="nofollow">[pdf]</a></li>
<li><strong>How transferable are features in deep neural networks?</strong> (2014), J. Yosinski et al. <a href="http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>CNN features off-the-Shelf: An astounding baseline for recognition</strong> (2014), A. Razavian et al. <a href="http://www.cv-foundation.org//openaccess/content_cvpr_workshops_2014/W15/papers/Razavian_CNN_Features_Off-the-Shelf_2014_CVPR_paper.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Learning and transferring mid-Level image representations using convolutional neural networks</strong> (2014), M. Oquab et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Visualizing and understanding convolutional networks</strong> (2014), M. Zeiler and R. Fergus <a href="http://arxiv.org/pdf/1311.2901" rel="nofollow">[pdf]</a></li>
<li><strong>Decaf: A deep convolutional activation feature for generic visual recognition</strong> (2014), J. Donahue et al. <a href="http://arxiv.org/pdf/1310.1531" rel="nofollow">[pdf]</a></li>
</ul>

<h3><a href="#optimization--training-techniques" aria-hidden="true" class="anchor" id="user-content-optimization--training-techniques"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Optimization / Training Techniques</h3>
<ul>
<li><strong>Training very deep networks</strong> (2015), R. Srivastava et al. <a href="http://papers.nips.cc/paper/5850-training-very-deep-networks.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Batch normalization: Accelerating deep network training by reducing internal covariate shift</strong> (2015), S. Loffe and C. Szegedy <a href="http://arxiv.org/pdf/1502.03167" rel="nofollow">[pdf]</a></li>
<li><strong>Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</strong> (2015), K. He et al. <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Dropout: A simple way to prevent neural networks from overfitting</strong> (2014), N. Srivastava et al. <a href="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Adam: A method for stochastic optimization</strong> (2014), D. Kingma and J. Ba <a href="http://arxiv.org/pdf/1412.6980" rel="nofollow">[pdf]</a></li>
<li><strong>Improving neural networks by preventing co-adaptation of feature detectors</strong> (2012), G. Hinton et al. <a href="http://arxiv.org/pdf/1207.0580.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Random search for hyper-parameter optimization</strong> (2012) J. Bergstra and Y. Bengio <a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a" rel="nofollow">[pdf]</a></li>
</ul>

<h3><a href="#unsupervised--generative-models" aria-hidden="true" class="anchor" id="user-content-unsupervised--generative-models"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Unsupervised / Generative Models</h3>
<ul>
<li><strong>Pixel recurrent neural networks</strong> (2016), A. Oord et al. <a href="http://arxiv.org/pdf/1601.06759v2.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Improved techniques for training GANs</strong> (2016), T. Salimans et al. <a href="http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Unsupervised representation learning with deep convolutional generative adversarial networks</strong> (2015), A. Radford et al. <a href="https://arxiv.org/pdf/1511.06434v2" rel="nofollow">[pdf]</a></li>
<li><strong>DRAW: A recurrent neural network for image generation</strong> (2015), K. Gregor et al. <a href="http://arxiv.org/pdf/1502.04623" rel="nofollow">[pdf]</a></li>
<li><strong>Generative adversarial nets</strong> (2014), I. Goodfellow et al. <a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Auto-encoding variational Bayes</strong> (2013), D. Kingma and M. Welling <a href="http://arxiv.org/pdf/1312.6114" rel="nofollow">[pdf]</a></li>
<li><strong>Building high-level features using large scale unsupervised learning</strong> (2013), Q. Le et al. <a href="http://arxiv.org/pdf/1112.6209" rel="nofollow">[pdf]</a></li>
</ul>

<h3><a href="#convolutional-neural-network-models" aria-hidden="true" class="anchor" id="user-content-convolutional-neural-network-models"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Convolutional Neural Network Models</h3>
<ul>
<li><strong>Rethinking the inception architecture for computer vision</strong> (2016), C. Szegedy et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Inception-v4, inception-resnet and the impact of residual connections on learning</strong> (2016), C. Szegedy et al. <a href="http://arxiv.org/pdf/1602.07261" rel="nofollow">[pdf]</a></li>
<li><strong>Identity Mappings in Deep Residual Networks</strong> (2016), K. He et al. <a href="https://arxiv.org/pdf/1603.05027v2.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Deep residual learning for image recognition</strong> (2016), K. He et al. <a href="http://arxiv.org/pdf/1512.03385" rel="nofollow">[pdf]</a></li>
<li><strong>Spatial transformer network</strong> (2015), M. Jaderberg et al., <a href="http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Going deeper with convolutions</strong> (2015), C. Szegedy et al.  <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Very deep convolutional networks for large-scale image recognition</strong> (2014), K. Simonyan and A. Zisserman <a href="http://arxiv.org/pdf/1409.1556" rel="nofollow">[pdf]</a></li>
<li><strong>Return of the devil in the details: delving deep into convolutional nets</strong> (2014), K. Chatfield et al. <a href="http://arxiv.org/pdf/1405.3531" rel="nofollow">[pdf]</a></li>
<li><strong>OverFeat: Integrated recognition, localization and detection using convolutional networks</strong> (2013), P. Sermanet et al. <a href="http://arxiv.org/pdf/1312.6229" rel="nofollow">[pdf]</a></li>
<li><strong>Maxout networks</strong> (2013), I. Goodfellow et al. <a href="http://arxiv.org/pdf/1302.4389v4" rel="nofollow">[pdf]</a></li>
<li><strong>Network in network</strong> (2013), M. Lin et al. <a href="http://arxiv.org/pdf/1312.4400" rel="nofollow">[pdf]</a></li>
<li><strong>ImageNet classification with deep convolutional neural networks</strong> (2012), A. Krizhevsky et al. <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="nofollow">[pdf]</a></li>
</ul>

<h3><a href="#image-segmentation--object-detection" aria-hidden="true" class="anchor" id="user-content-image-segmentation--object-detection"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Image: Segmentation / Object Detection</h3>
<ul>
<li><strong>You only look once: Unified, real-time object detection</strong> (2016), J. Redmon et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Fully convolutional networks for semantic segmentation</strong> (2015), J. Long et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</strong> (2015), S. Ren et al. <a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Fast R-CNN</strong> (2015), R. Girshick <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Rich feature hierarchies for accurate object detection and semantic segmentation</strong> (2014), R. Girshick et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Spatial pyramid pooling in deep convolutional networks for visual recognition</strong> (2014), K. He et al. <a href="http://arxiv.org/pdf/1406.4729" rel="nofollow">[pdf]</a></li>
<li><strong>Semantic image segmentation with deep convolutional nets and fully connected CRFs</strong>, L. Chen et al. <a href="https://arxiv.org/pdf/1412.7062" rel="nofollow">[pdf]</a></li>
<li><strong>Learning hierarchical features for scene labeling</strong> (2013), C. Farabet et al. <a href="https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf" rel="nofollow">[pdf]</a></li>
</ul>

<h3><a href="#image--video--etc" aria-hidden="true" class="anchor" id="user-content-image--video--etc"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Image / Video / Etc</h3>
<ul>
<li><strong>Image Super-Resolution Using Deep Convolutional Networks</strong> (2016), C. Dong et al. <a href="https://arxiv.org/pdf/1501.00092v3.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>A neural algorithm of artistic style</strong> (2015), L. Gatys et al. <a href="https://arxiv.org/pdf/1508.06576" rel="nofollow">[pdf]</a></li>
<li><strong>Deep visual-semantic alignments for generating image descriptions</strong> (2015), A. Karpathy and L. Fei-Fei <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Show, attend and tell: Neural image caption generation with visual attention</strong> (2015), K. Xu et al. <a href="http://arxiv.org/pdf/1502.03044" rel="nofollow">[pdf]</a></li>
<li><strong>Show and tell: A neural image caption generator</strong> (2015), O. Vinyals et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Long-term recurrent convolutional networks for visual recognition and description</strong> (2015), J. Donahue et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>VQA: Visual question answering</strong> (2015), S. Antol et al. <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>DeepFace: Closing the gap to human-level performance in face verification</strong> (2014), Y. Taigman et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf" rel="nofollow">[pdf]</a>:</li>
<li><strong>Large-scale video classification with convolutional neural networks</strong> (2014), A. Karpathy et al. <a href="http://vision.stanford.edu/pdf/karpathy14.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Two-stream convolutional networks for action recognition in videos</strong> (2014), K. Simonyan et al. <a href="http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>3D convolutional neural networks for human action recognition</strong> (2013), S. Ji et al. <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_JiXYY10.pdf" rel="nofollow">[pdf]</a></li>
</ul>


<h3><a href="#natural-language-processing--rnns" aria-hidden="true" class="anchor" id="user-content-natural-language-processing--rnns"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Natural Language Processing / RNNs</h3>
<ul>
<li><strong>Neural Architectures for Named Entity Recognition</strong> (2016), G. Lample et al. <a href="http://aclweb.org/anthology/N/N16/N16-1030.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Exploring the limits of language modeling</strong> (2016), R. Jozefowicz et al. <a href="http://arxiv.org/pdf/1602.02410" rel="nofollow">[pdf]</a></li>
<li><strong>Teaching machines to read and comprehend</strong> (2015), K. Hermann et al. <a href="http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Effective approaches to attention-based neural machine translation</strong> (2015), M. Luong et al. <a href="https://arxiv.org/pdf/1508.04025" rel="nofollow">[pdf]</a></li>
<li><strong>Conditional random fields as recurrent neural networks</strong> (2015), S. Zheng and S. Jayasumana. <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Conditional_Random_Fields_ICCV_2015_paper.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Memory networks</strong> (2014), J. Weston et al. <a href="https://arxiv.org/pdf/1410.3916" rel="nofollow">[pdf]</a></li>
<li><strong>Neural turing machines</strong> (2014), A. Graves et al. <a href="https://arxiv.org/pdf/1410.5401" rel="nofollow">[pdf]</a></li>
<li><strong>Neural machine translation by jointly learning to align and translate</strong> (2014), D. Bahdanau et al. <a href="http://arxiv.org/pdf/1409.0473" rel="nofollow">[pdf]</a></li>
<li><strong>Sequence to sequence learning with neural networks</strong> (2014), I. Sutskever et al. <a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Learning phrase representations using RNN encoder-decoder for statistical machine translation</strong> (2014), K. Cho et al. <a href="http://arxiv.org/pdf/1406.1078" rel="nofollow">[pdf]</a></li>
<li><strong>A convolutional neural network for modeling sentences</strong> (2014), N. Kalchbrenner et al. <a href="http://arxiv.org/pdf/1404.2188v1" rel="nofollow">[pdf]</a></li>
<li><strong>Convolutional neural networks for sentence classification</strong> (2014), Y. Kim <a href="http://arxiv.org/pdf/1408.5882" rel="nofollow">[pdf]</a></li>
<li><strong>Glove: Global vectors for word representation</strong> (2014), J. Pennington et al. <a href="http://anthology.aclweb.org/D/D14/D14-1162.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Distributed representations of sentences and documents</strong> (2014), Q. Le and T. Mikolov <a href="http://arxiv.org/pdf/1405.4053" rel="nofollow">[pdf]</a></li>
<li><strong>Distributed representations of words and phrases and their compositionality</strong> (2013), T. Mikolov et al. <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Efficient estimation of word representations in vector space</strong> (2013), T. Mikolov et al.  <a href="http://arxiv.org/pdf/1301.3781" rel="nofollow">[pdf]</a></li>
<li><strong>Recursive deep models for semantic compositionality over a sentiment treebank</strong> (2013), R. Socher et al. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.1327&amp;rep=rep1&amp;type=pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Generating sequences with recurrent neural networks</strong> (2013), A. Graves. <a href="https://arxiv.org/pdf/1308.0850" rel="nofollow">[pdf]</a></li>
</ul>

<h3><a href="#speech--other-domain" aria-hidden="true" class="anchor" id="user-content-speech--other-domain"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Speech / Other Domain</h3>
<ul>
<li><strong>End-to-end attention-based large vocabulary speech recognition</strong> (2016), D. Bahdanau et al. <a href="https://arxiv.org/pdf/1508.04395" rel="nofollow">[pdf]</a></li>
<li><strong>Deep speech 2: End-to-end speech recognition in English and Mandarin</strong> (2015), D. Amodei et al. <a href="https://arxiv.org/pdf/1512.02595" rel="nofollow">[pdf]</a></li>
<li><strong>Speech recognition with deep recurrent neural networks</strong> (2013), A. Graves <a href="http://arxiv.org/pdf/1303.5778.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</strong> (2012), G. Hinton et al. <a href="http://www.cs.toronto.edu/%7Easamir/papers/SPM_DNN_12.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</strong> (2012) G. Dahl et al. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.337.7548&amp;rep=rep1&amp;type=pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Acoustic modeling using deep belief networks</strong> (2012), A. Mohamed et al. <a href="http://www.cs.toronto.edu/%7Easamir/papers/speechDBN_jrnl.pdf" rel="nofollow">[pdf]</a></li>
</ul>

<h3><a href="#reinforcement-learning--robotics" aria-hidden="true" class="anchor" id="user-content-reinforcement-learning--robotics"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Reinforcement Learning / Robotics</h3>
<ul>
<li><strong>End-to-end training of deep visuomotor policies</strong> (2016), S. Levine et al. <a href="http://www.jmlr.org/papers/volume17/15-522/source/15-522.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection</strong> (2016), S. Levine et al. <a href="https://arxiv.org/pdf/1603.02199" rel="nofollow">[pdf]</a></li>
<li><strong>Asynchronous methods for deep reinforcement learning</strong> (2016), V. Mnih et al. <a href="http://www.jmlr.org/proceedings/papers/v48/mniha16.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Deep Reinforcement Learning with Double Q-Learning</strong> (2016), H. Hasselt et al. <a href="https://arxiv.org/pdf/1509.06461.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Mastering the game of Go with deep neural networks and tree search</strong> (2016), D. Silver et al. <a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html" rel="nofollow">[pdf]</a></li>
<li><strong>Continuous control with deep reinforcement learning</strong> (2015), T. Lillicrap et al. <a href="https://arxiv.org/pdf/1509.02971" rel="nofollow">[pdf]</a></li>
<li><strong>Human-level control through deep reinforcement learning</strong> (2015), V. Mnih et al. <a href="http://www.davidqiu.com:8888/research/nature14236.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Deep learning for detecting robotic grasps</strong> (2015), I. Lenz et al. <a href="http://www.cs.cornell.edu/%7Easaxena/papers/lenz_lee_saxena_deep_learning_grasping_ijrr2014.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Playing atari with deep reinforcement learning</strong> (2013), V. Mnih et al. <a href="http://arxiv.org/pdf/1312.5602.pdf" rel="nofollow">[pdf]</a>)</li>
</ul>

<h3><a href="#more-papers-from-2016" aria-hidden="true" class="anchor" id="user-content-more-papers-from-2016"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>More Papers from 2016</h3>
<ul>
<li><strong>Layer Normalization</strong> (2016), J. Ba et al. <a href="https://arxiv.org/pdf/1607.06450v1.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Learning to learn by gradient descent by gradient descent</strong> (2016), M. Andrychowicz et al. <a href="http://arxiv.org/pdf/1606.04474v1" rel="nofollow">[pdf]</a></li>
<li><strong>Domain-adversarial training of neural networks</strong> (2016), Y. Ganin et al. <a href="http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>WaveNet: A Generative Model for Raw Audio</strong> (2016), A. Oord et al. <a href="https://arxiv.org/pdf/1609.03499v2" rel="nofollow">[pdf]</a> <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" rel="nofollow">[web]</a></li>
<li><strong>Colorful image colorization</strong> (2016), R. Zhang et al. <a href="https://arxiv.org/pdf/1603.08511" rel="nofollow">[pdf]</a></li>
<li><strong>Generative visual manipulation on the natural image manifold</strong> (2016), J. Zhu et al. <a href="https://arxiv.org/pdf/1609.03552" rel="nofollow">[pdf]</a></li>
<li><strong>Texture networks: Feed-forward synthesis of textures and stylized images</strong> (2016), D Ulyanov et al. <a href="http://www.jmlr.org/proceedings/papers/v48/ulyanov16.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>SSD: Single shot multibox detector</strong> (2016), W. Liu et al. <a href="https://arxiv.org/pdf/1512.02325" rel="nofollow">[pdf]</a></li>
<li><strong>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 1MB model size</strong> (2016), F. Iandola et al. <a href="http://arxiv.org/pdf/1602.07360" rel="nofollow">[pdf]</a></li>
<li><strong>Eie: Efficient inference engine on compressed deep neural network</strong> (2016), S. Han et al. <a href="http://arxiv.org/pdf/1602.01528" rel="nofollow">[pdf]</a></li>
<li><strong>Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1</strong> (2016), M. Courbariaux et al. <a href="https://arxiv.org/pdf/1602.02830" rel="nofollow">[pdf]</a></li>
<li><strong>Dynamic memory networks for visual and textual question answering</strong> (2016), C. Xiong et al. <a href="http://www.jmlr.org/proceedings/papers/v48/xiong16.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Stacked attention networks for image question answering</strong> (2016), Z. Yang et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Stacked_Attention_Networks_CVPR_2016_paper.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Hybrid computing using a neural network with dynamic external memory</strong> (2016), A. Graves et al. <a href="https://www.gwern.net/docs/2016-graves.pdf" rel="nofollow">[pdf]</a></li>
<li><strong>Google's neural machine translation system: Bridging the gap between human and machine translation</strong> (2016), Y. Wu et al. <a href="https://arxiv.org/pdf/1609.08144" rel="nofollow">[pdf]</a></li>
</ul>
<hr>
<h3><a href="#new-papers" aria-hidden="true" class="anchor" id="user-content-new-papers"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>New papers</h3>
<p><em>Newly published papers (&lt; 6 months) which are worth reading</em></p>
<ul>
<li>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (2017), Andrew G. Howard et al. <a href="https://arxiv.org/pdf/1704.04861.pdf" rel="nofollow">[pdf]</a></li>
<li>Convolutional Sequence to Sequence Learning (2017), Jonas Gehring et al. <a href="https://arxiv.org/pdf/1705.03122" rel="nofollow">[pdf]</a></li>
<li>A Knowledge-Grounded Neural Conversation Model (2017), Marjan Ghazvininejad et al. <a href="https://arxiv.org/pdf/1702.01932" rel="nofollow">[pdf]</a></li>
<li>Accurate, Large Minibatch SGD:Training ImageNet in 1 Hour (2017), Priya Goyal et al. <a href="https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h3.pdf" rel="nofollow">[pdf]</a></li>
<li>TACOTRON: Towards end-to-end speech synthesis (2017), Y. Wang et al. <a href="https://arxiv.org/pdf/1703.10135.pdf" rel="nofollow">[pdf]</a></li>
<li>Deep Photo Style Transfer (2017), F. Luan et al. <a href="http://arxiv.org/pdf/1703.07511v1.pdf" rel="nofollow">[pdf]</a></li>
<li>Evolution Strategies as a Scalable Alternative to Reinforcement Learning (2017), T. Salimans et al. <a href="http://arxiv.org/pdf/1703.03864v1.pdf" rel="nofollow">[pdf]</a></li>
<li>Deformable Convolutional Networks (2017), J. Dai et al. <a href="http://arxiv.org/pdf/1703.06211v2.pdf" rel="nofollow">[pdf]</a></li>
<li>Mask R-CNN (2017), K. He et al. <a href="https://128.84.21.199/pdf/1703.06870" rel="nofollow">[pdf]</a></li>
<li>Learning to discover cross-domain relations with generative adversarial networks (2017), T. Kim et al. <a href="http://arxiv.org/pdf/1703.05192v1.pdf" rel="nofollow">[pdf]</a></li>
<li>Deep voice: Real-time neural text-to-speech (2017), S. Arik et al., <a href="http://arxiv.org/pdf/1702.07825v2.pdf" rel="nofollow">[pdf]</a></li>
<li>PixelNet: Representation of the pixels, by the pixels, and for the pixels (2017), A. Bansal et al. <a href="http://arxiv.org/pdf/1702.06506v1.pdf" rel="nofollow">[pdf]</a></li>
<li>Batch renormalization: Towards reducing minibatch dependence in batch-normalized models (2017), S. Ioffe. <a href="https://arxiv.org/abs/1702.03275" rel="nofollow">[pdf]</a></li>
<li>Wasserstein GAN (2017), M. Arjovsky et al. <a href="https://arxiv.org/pdf/1701.07875v1" rel="nofollow">[pdf]</a></li>
<li>Understanding deep learning requires rethinking generalization (2017), C. Zhang et al. <a href="https://arxiv.org/pdf/1611.03530" rel="nofollow">[pdf]</a></li>
<li>Least squares generative adversarial networks (2016), X. Mao et al. <a href="https://arxiv.org/abs/1611.04076v2" rel="nofollow">[pdf]</a></li>
</ul>
<h3><a href="#old-papers" aria-hidden="true" class="anchor" id="user-content-old-papers"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Old Papers</h3>
<p><em>Classic papers published before 2012</em></p>
<ul>
<li>An analysis of single-layer networks in unsupervised feature learning (2011), A. Coates et al. <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf" rel="nofollow">[pdf]</a></li>
<li>Deep sparse rectifier neural networks (2011), X. Glorot et al. <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf" rel="nofollow">[pdf]</a></li>
<li>Natural language processing (almost) from scratch (2011), R. Collobert et al. <a href="http://arxiv.org/pdf/1103.0398" rel="nofollow">[pdf]</a></li>
<li>Recurrent neural network based language model (2010), T. Mikolov et al. <a href="http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf" rel="nofollow">[pdf]</a></li>
<li>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion (2010), P. Vincent et al. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3484&amp;rep=rep1&amp;type=pdf" rel="nofollow">[pdf]</a></li>
<li>Learning mid-level features for recognition (2010), Y. Boureau <a href="http://ece.duke.edu/%7Elcarin/boureau-cvpr-10.pdf" rel="nofollow">[pdf]</a></li>
<li>A practical guide to training restricted boltzmann machines (2010), G. Hinton <a href="http://www.csri.utoronto.ca/%7Ehinton/absps/guideTR.pdf" rel="nofollow">[pdf]</a></li>
<li>Understanding the difficulty of training deep feedforward neural networks (2010), X. Glorot and Y. Bengio <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf" rel="nofollow">[pdf]</a></li>
<li>Why does unsupervised pre-training help deep learning (2010), D. Erhan et al. <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_ErhanCBV10.pdf" rel="nofollow">[pdf]</a></li>
<li>Learning deep architectures for AI (2009), Y. Bengio. <a href="http://sanghv.com/download/soft/machine%20learning,%20artificial%20intelligence,%20mathematics%20ebooks/ML/learning%20deep%20architectures%20for%20AI%20(2009).pdf" rel="nofollow">[pdf]</a></li>
<li>Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations (2009), H. Lee et al. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.802&amp;rep=rep1&amp;type=pdf" rel="nofollow">[pdf]</a></li>
<li>Greedy layer-wise training of deep networks (2007), Y. Bengio et al. <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_739.pdf" rel="nofollow">[pdf]</a></li>
<li>Reducing the dimensionality of data with neural networks, G. Hinton and R. Salakhutdinov. <a href="http://homes.mpimf-heidelberg.mpg.de/%7Emhelmsta/pdf/2006%20Hinton%20Salakhudtkinov%20Science.pdf" rel="nofollow">[pdf]</a></li>
<li>A fast learning algorithm for deep belief nets (2006), G. Hinton et al. <a href="http://nuyoo.utm.mx/%7Ejjf/rna/A8%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets.pdf" rel="nofollow">[pdf]</a></li>
<li>Gradient-based learning applied to document recognition (1998), Y. LeCun et al. <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="nofollow">[pdf]</a></li>
<li>Long short-term memory (1997), S. Hochreiter and J. Schmidhuber. <a href="http://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.1997.9.8.1735" rel="nofollow">[pdf]</a></li>
</ul>
<h3><a href="#hw--sw--dataset" aria-hidden="true" class="anchor" id="user-content-hw--sw--dataset"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>HW / SW / Dataset</h3>
<ul>
<li>SQuAD: 100,000+ Questions for Machine Comprehension of Text (2016), Rajpurkar et al. <a href="https://arxiv.org/pdf/1606.05250.pdf" rel="nofollow">[pdf]</a></li>
<li>OpenAI gym (2016), G. Brockman et al. <a href="https://arxiv.org/pdf/1606.01540" rel="nofollow">[pdf]</a></li>
<li>TensorFlow: Large-scale machine learning on heterogeneous distributed systems (2016), M. Abadi et al. <a href="http://arxiv.org/pdf/1603.04467" rel="nofollow">[pdf]</a></li>
<li>Theano: A Python framework for fast computation of mathematical expressions, R. Al-Rfou et al.</li>
<li>Torch7: A matlab-like environment for machine learning, R. Collobert et al. <a href="https://ronan.collobert.com/pub/matos/2011_torch7_nipsw.pdf" rel="nofollow">[pdf]</a></li>
<li>MatConvNet: Convolutional neural networks for matlab (2015), A. Vedaldi and K. Lenc <a href="http://arxiv.org/pdf/1412.4564" rel="nofollow">[pdf]</a></li>
<li>Imagenet large scale visual recognition challenge (2015), O. Russakovsky et al. <a href="http://arxiv.org/pdf/1409.0575" rel="nofollow">[pdf]</a></li>
<li>Caffe: Convolutional architecture for fast feature embedding (2014), Y. Jia et al. <a href="http://arxiv.org/pdf/1408.5093" rel="nofollow">[pdf]</a></li>
</ul>
<h3><a href="#book--survey--review" aria-hidden="true" class="anchor" id="user-content-book--survey--review"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Book / Survey / Review</h3>
<ul>
<li>On the Origin of Deep Learning (2017), H. Wang and Bhiksha Raj. <a href="https://arxiv.org/pdf/1702.07800" rel="nofollow">[pdf]</a></li>
<li>Deep Reinforcement Learning: An Overview (2017), Y. Li, <a href="http://arxiv.org/pdf/1701.07274v2.pdf" rel="nofollow">[pdf]</a></li>
<li>Neural Machine Translation and Sequence-to-sequence Models(2017): A Tutorial, G. Neubig. <a href="http://arxiv.org/pdf/1703.01619v1.pdf" rel="nofollow">[pdf]</a></li>
<li>Neural Network and Deep Learning (Book, Jan 2017), Michael Nielsen. <a href="http://neuralnetworksanddeeplearning.com/index.html" rel="nofollow">[html]</a></li>
<li>Deep learning (Book, 2016), Goodfellow et al. <a href="http://www.deeplearningbook.org/" rel="nofollow">[html]</a></li>
<li>LSTM: A search space odyssey (2016), K. Greff et al. <a href="https://arxiv.org/pdf/1503.04069.pdf?utm_content=buffereddc5&amp;utm_medium=social&amp;utm_source=plus.google.com&amp;utm_campaign=buffer" rel="nofollow">[pdf]</a></li>
<li>Tutorial on Variational Autoencoders (2016), C. Doersch. <a href="https://arxiv.org/pdf/1606.05908" rel="nofollow">[pdf]</a></li>
<li>Deep learning (2015), Y. LeCun, Y. Bengio and G. Hinton <a href="https://www.cs.toronto.edu/%7Ehinton/absps/NatureDeepReview.pdf" rel="nofollow">[pdf]</a></li>
<li>Deep learning in neural networks: An overview (2015), J. Schmidhuber <a href="http://arxiv.org/pdf/1404.7828" rel="nofollow">[pdf]</a></li>
<li>Representation learning: A review and new perspectives (2013), Y. Bengio et al. <a href="http://arxiv.org/pdf/1206.5538" rel="nofollow">[pdf]</a></li>
</ul>
<h3><a href="#video-lectures--tutorials--blogs" aria-hidden="true" class="anchor" id="user-content-video-lectures--tutorials--blogs"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Video Lectures / Tutorials / Blogs</h3>
<p><em>(Lectures)</em></p>
<ul>
<li>CS231n, Convolutional Neural Networks for Visual Recognition, Stanford University <a href="http://cs231n.stanford.edu/" rel="nofollow">[web]</a></li>
<li>CS224d, Deep Learning for Natural Language Processing, Stanford University <a href="http://cs224d.stanford.edu/" rel="nofollow">[web]</a></li>
<li>Oxford Deep NLP 2017, Deep Learning for Natural Language Processing, University of Oxford <a href="https://github.com/oxford-cs-deepnlp-2017/lectures">[web]</a></li>
</ul>
<p><em>(Tutorials)</em></p>
<ul>
<li>NIPS 2016 Tutorials, Long Beach <a href="https://nips.cc/Conferences/2016/Schedule?type=Tutorial" rel="nofollow">[web]</a></li>
<li>ICML 2016 Tutorials, New York City <a href="http://techtalks.tv/icml/2016/tutorials/" rel="nofollow">[web]</a></li>
<li>ICLR 2016 Videos, San Juan <a href="http://videolectures.net/iclr2016_san_juan/" rel="nofollow">[web]</a></li>
<li>Deep Learning Summer School 2016, Montreal <a href="http://videolectures.net/deeplearning2016_montreal/" rel="nofollow">[web]</a></li>
<li>Bay Area Deep Learning School 2016, Stanford <a href="https://www.bayareadlschool.org/" rel="nofollow">[web]</a></li>
</ul>
<p><em>(Blogs)</em></p>
<ul>
<li>OpenAI <a href="https://www.openai.com/" rel="nofollow">[web]</a></li>
<li>Distill <a href="http://distill.pub/" rel="nofollow">[web]</a></li>
<li>Andrej Karpathy Blog <a href="http://karpathy.github.io/" rel="nofollow">[web]</a></li>
<li>Colah's Blog <a href="http://colah.github.io/" rel="nofollow">[Web]</a></li>
<li>WildML <a href="http://www.wildml.com/" rel="nofollow">[Web]</a></li>
<li>FastML <a href="http://www.fastml.com/" rel="nofollow">[web]</a></li>
<li>TheMorningPaper <a href="https://blog.acolyer.org" rel="nofollow">[web]</a></li>
</ul>
<h3><a href="#appendix-more-than-top-100" aria-hidden="true" class="anchor" id="user-content-appendix-more-than-top-100"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Appendix: More than Top 100</h3>
<p><em>(2016)</em></p>
<ul>
<li>A character-level decoder without explicit segmentation for neural machine translation (2016), J. Chung et al. <a href="https://arxiv.org/pdf/1603.06147" rel="nofollow">[pdf]</a></li>
<li>Dermatologist-level classification of skin cancer with deep neural networks (2017), A. Esteva et al. <a href="http://www.nature.com/nature/journal/v542/n7639/full/nature21056.html" rel="nofollow">[html]</a></li>
<li>Weakly supervised object localization with multi-fold multiple instance learning (2017), R. Gokberk et al. <a href="https://arxiv.org/pdf/1503.00949" rel="nofollow">[pdf]</a></li>
<li>Brain tumor segmentation with deep neural networks (2017), M. Havaei et al. <a href="https://arxiv.org/pdf/1505.03540" rel="nofollow">[pdf]</a></li>
<li>Professor Forcing: A New Algorithm for Training Recurrent Networks (2016), A. Lamb et al. <a href="https://arxiv.org/pdf/1610.09038" rel="nofollow">[pdf]</a></li>
<li>Adversarially learned inference (2016), V. Dumoulin et al. <a href="https://ishmaelbelghazi.github.io/ALI/" rel="nofollow">[web]</a><a href="https://arxiv.org/pdf/1606.00704v1" rel="nofollow">[pdf]</a></li>
<li>Understanding convolutional neural networks (2016), J. Koushik <a href="https://arxiv.org/pdf/1605.09081v1" rel="nofollow">[pdf]</a></li>
<li>Taking the human out of the loop: A review of bayesian optimization (2016), B. Shahriari et al. <a href="https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf" rel="nofollow">[pdf]</a></li>
<li>Adaptive computation time for recurrent neural networks (2016), A. Graves <a href="http://arxiv.org/pdf/1603.08983" rel="nofollow">[pdf]</a></li>
<li>Densely connected convolutional networks (2016), G. Huang et al. <a href="https://arxiv.org/pdf/1608.06993v1" rel="nofollow">[pdf]</a></li>
<li>Region-based convolutional networks for accurate object detection and segmentation (2016), R. Girshick et al.</li>
<li>Continuous deep q-learning with model-based acceleration (2016), S. Gu et al. <a href="http://www.jmlr.org/proceedings/papers/v48/gu16.pdf" rel="nofollow">[pdf]</a></li>
<li>A thorough examination of the cnn/daily mail reading comprehension task (2016), D. Chen et al. <a href="https://arxiv.org/pdf/1606.02858" rel="nofollow">[pdf]</a></li>
<li>Achieving open vocabulary neural machine translation with hybrid word-character models, M. Luong and C. Manning. <a href="https://arxiv.org/pdf/1604.00788" rel="nofollow">[pdf]</a></li>
<li>Very Deep Convolutional Networks for Natural Language Processing (2016), A. Conneau et al. <a href="https://arxiv.org/pdf/1606.01781" rel="nofollow">[pdf]</a></li>
<li>Bag of tricks for efficient text classification (2016), A. Joulin et al. <a href="https://arxiv.org/pdf/1607.01759" rel="nofollow">[pdf]</a></li>
<li>Efficient piecewise training of deep structured models for semantic segmentation (2016), G. Lin et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lin_Efficient_Piecewise_Training_CVPR_2016_paper.pdf" rel="nofollow">[pdf]</a></li>
<li>Learning to compose neural networks for question answering (2016), J. Andreas et al. <a href="https://arxiv.org/pdf/1601.01705" rel="nofollow">[pdf]</a></li>
<li>Perceptual losses for real-time style transfer and super-resolution (2016), J. Johnson et al. <a href="https://arxiv.org/pdf/1603.08155" rel="nofollow">[pdf]</a></li>
<li>Reading text in the wild with convolutional neural networks (2016), M. Jaderberg et al. <a href="http://arxiv.org/pdf/1412.1842" rel="nofollow">[pdf]</a></li>
<li>What makes for effective detection proposals? (2016), J. Hosang et al. <a href="https://arxiv.org/pdf/1502.05082" rel="nofollow">[pdf]</a></li>
<li>Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks (2016), S. Bell et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Bell_Inside-Outside_Net_Detecting_CVPR_2016_paper.pdf" rel="nofollow">[pdf]</a>.</li>
<li>Instance-aware semantic segmentation via multi-task network cascades (2016), J. Dai et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Dai_Instance-Aware_Semantic_Segmentation_CVPR_2016_paper.pdf" rel="nofollow">[pdf]</a></li>
<li>Conditional image generation with pixelcnn decoders (2016), A. van den Oord et al. <a href="http://papers.nips.cc/paper/6527-tree-structured-reinforcement-learning-for-sequential-object-localization.pdf" rel="nofollow">[pdf]</a></li>
<li>Deep networks with stochastic depth (2016), G. Huang et al., <a href="https://arxiv.org/pdf/1603.09382" rel="nofollow">[pdf]</a></li>
<li>Consistency and Fluctuations For Stochastic Gradient Langevin Dynamics (2016), Yee Whye Teh et al. <a href="http://www.jmlr.org/papers/volume17/teh16a/teh16a.pdf" rel="nofollow">[pdf]</a></li>
</ul>
<p><em>(2015)</em></p>
<ul>
<li>Ask your neurons: A neural-based approach to answering questions about images (2015), M. Malinowski et al. <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Malinowski_Ask_Your_Neurons_ICCV_2015_paper.pdf" rel="nofollow">[pdf]</a></li>
<li>Exploring models and data for image question answering (2015), M. Ren et al. <a href="http://papers.nips.cc/paper/5640-stochastic-variational-inference-for-hidden-markov-models.pdf" rel="nofollow">[pdf]</a></li>
<li>Are you talking to a machine? dataset and methods for multilingual image question (2015), H. Gao et al. <a href="http://papers.nips.cc/paper/5641-are-you-talking-to-a-machine-dataset-and-methods-for-multilingual-image-question.pdf" rel="nofollow">[pdf]</a></li>
<li>Mind's eye: A recurrent visual representation for image caption generation (2015), X. Chen and C. Zitnick. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Chen_Minds_Eye_A_2015_CVPR_paper.pdf" rel="nofollow">[pdf]</a></li>
<li>From captions to visual concepts and back (2015), H. Fang et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Fang_From_Captions_to_2015_CVPR_paper.pdf" rel="nofollow">[pdf]</a>.</li>
<li>Towards AI-complete question answering: A set of prerequisite toy tasks (2015), J. Weston et al. <a href="http://arxiv.org/pdf/1502.05698" rel="nofollow">[pdf]</a></li>
<li>Ask me anything: Dynamic memory networks for natural language processing (2015), A. Kumar et al. <a href="http://arxiv.org/pdf/1506.07285" rel="nofollow">[pdf]</a></li>
<li>Unsupervised learning of video representations using LSTMs (2015), N. Srivastava et al. <a href="http://www.jmlr.org/proceedings/papers/v37/srivastava15.pdf" rel="nofollow">[pdf]</a></li>
<li>Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding (2015), S. Han et al. <a href="https://arxiv.org/pdf/1510.00149" rel="nofollow">[pdf]</a></li>
<li>Improved semantic representations from tree-structured long short-term memory networks (2015), K. Tai et al. <a href="https://arxiv.org/pdf/1503.00075" rel="nofollow">[pdf]</a></li>
<li>Character-aware neural language models (2015), Y. Kim et al. <a href="https://arxiv.org/pdf/1508.06615" rel="nofollow">[pdf]</a></li>
<li>Grammar as a foreign language (2015), O. Vinyals et al. <a href="http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf" rel="nofollow">[pdf]</a></li>
<li>Trust Region Policy Optimization (2015), J. Schulman et al. <a href="http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf" rel="nofollow">[pdf]</a></li>
<li>Beyond short snippents: Deep networks for video classification (2015) <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf" rel="nofollow">[pdf]</a></li>
<li>Learning Deconvolution Network for Semantic Segmentation (2015), H. Noh et al. <a href="https://arxiv.org/pdf/1505.04366v1" rel="nofollow">[pdf]</a></li>
<li>Learning spatiotemporal features with 3d convolutional networks (2015), D. Tran et al. <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf" rel="nofollow">[pdf]</a></li>
<li>Understanding neural networks through deep visualization (2015), J. Yosinski et al. <a href="https://arxiv.org/pdf/1506.06579" rel="nofollow">[pdf]</a></li>
<li>An Empirical Exploration of Recurrent Network Architectures (2015), R. Jozefowicz et al.  <a href="http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" rel="nofollow">[pdf]</a></li>
<li>Deep generative image models using aï¿¼ laplacian pyramid of adversarial networks (2015), E.Denton et al. <a href="http://papers.nips.cc/paper/5773-deep-generative-image-models-using-a-laplacian-pyramid-of-adversarial-networks.pdf" rel="nofollow">[pdf]</a></li>
<li>Gated Feedback Recurrent Neural Networks (2015), J. Chung et al. <a href="http://www.jmlr.org/proceedings/papers/v37/chung15.pdf" rel="nofollow">[pdf]</a></li>
<li>Fast and accurate deep network learning by exponential linear units (ELUS) (2015), D. Clevert et al. <a href="https://arxiv.org/pdf/1511.07289.pdf%5Cnhttp://arxiv.org/abs/1511.07289%5Cnhttp://arxiv.org/abs/1511.07289" rel="nofollow">[pdf]</a></li>
<li>Pointer networks (2015), O. Vinyals et al. <a href="http://papers.nips.cc/paper/5866-pointer-networks.pdf" rel="nofollow">[pdf]</a></li>
<li>Visualizing and Understanding Recurrent Networks (2015), A. Karpathy et al. <a href="https://arxiv.org/pdf/1506.02078" rel="nofollow">[pdf]</a></li>
<li>Attention-based models for speech recognition (2015), J. Chorowski et al. <a href="http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf" rel="nofollow">[pdf]</a></li>
<li>End-to-end memory networks (2015), S. Sukbaatar et al. <a href="http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" rel="nofollow">[pdf]</a></li>
<li>Describing videos by exploiting temporal structure (2015), L. Yao et al. <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yao_Describing_Videos_by_ICCV_2015_paper.pdf" rel="nofollow">[pdf]</a></li>
<li>A neural conversational model (2015), O. Vinyals and Q. Le. <a href="https://arxiv.org/pdf/1506.05869.pdf" rel="nofollow">[pdf]</a></li>
<li>Improving distributional similarity with lessons learned from word embeddings, O. Levy et al. [[pdf]] (<a href="https://www.transacl.org/ojs/index.php/tacl/article/download/570/124" rel="nofollow">https://www.transacl.org/ojs/index.php/tacl/article/download/570/124</a>)</li>
<li>Transition-Based Dependency Parsing with Stack Long Short-Term Memory (2015), C. Dyer et al. <a href="http://aclweb.org/anthology/P/P15/P15-1033.pdf" rel="nofollow">[pdf]</a></li>
<li>Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs (2015), M. Ballesteros et al. <a href="http://aclweb.org/anthology/D/D15/D15-1041.pdf" rel="nofollow">[pdf]</a></li>
<li>Finding function in form: Compositional character models for open vocabulary word representation (2015), W. Ling et al. <a href="http://aclweb.org/anthology/D/D15/D15-1176.pdf" rel="nofollow">[pdf]</a></li>
</ul>
<p><em>(~2014)</em></p>
<ul>
<li>DeepPose: Human pose estimation via deep neural networks (2014), A. Toshev and C. Szegedy <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Toshev_DeepPose_Human_Pose_2014_CVPR_paper.pdf" rel="nofollow">[pdf]</a></li>
<li>Learning a Deep Convolutional Network for Image Super-Resolution (2014, C. Dong et al. <a href="https://www.researchgate.net/profile/Chen_Change_Loy/publication/264552416_Lecture_Notes_in_Computer_Science/links/53e583e50cf25d674e9c280e.pdf" rel="nofollow">[pdf]</a></li>
<li>Recurrent models of visual attention (2014), V. Mnih et al. <a href="http://arxiv.org/pdf/1406.6247.pdf" rel="nofollow">[pdf]</a></li>
<li>Empirical evaluation of gated recurrent neural networks on sequence modeling (2014), J. Chung et al. <a href="https://arxiv.org/pdf/1412.3555" rel="nofollow">[pdf]</a></li>
<li>Addressing the rare word problem in neural machine translation (2014), M. Luong et al. <a href="https://arxiv.org/pdf/1410.8206" rel="nofollow">[pdf]</a></li>
<li>On the properties of neural machine translation: Encoder-decoder approaches (2014), K. Cho et. al.</li>
<li>Recurrent neural network regularization (2014), W. Zaremba et al. <a href="http://arxiv.org/pdf/1409.2329" rel="nofollow">[pdf]</a></li>
<li>Intriguing properties of neural networks (2014), C. Szegedy et al. <a href="https://arxiv.org/pdf/1312.6199.pdf" rel="nofollow">[pdf]</a></li>
<li>Towards end-to-end speech recognition with recurrent neural networks (2014), A. Graves and N. Jaitly. <a href="http://www.jmlr.org/proceedings/papers/v32/graves14.pdf" rel="nofollow">[pdf]</a></li>
<li>Scalable object detection using deep neural networks (2014), D. Erhan et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Erhan_Scalable_Object_Detection_2014_CVPR_paper.pdf" rel="nofollow">[pdf]</a></li>
<li>On the importance of initialization and momentum in deep learning (2013), I. Sutskever et al. <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_sutskever13.pdf" rel="nofollow">[pdf]</a></li>
<li>Regularization of neural networks using dropconnect (2013), L. Wan et al. <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_wan13.pdf" rel="nofollow">[pdf]</a></li>
<li>Learning Hierarchical Features for Scene Labeling (2013), C. Farabet et al. <a href="https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf" rel="nofollow">[pdf]</a></li>
<li>Linguistic Regularities in Continuous Space Word Representations (2013), T. Mikolov et al. <a href="http://www.aclweb.org/anthology/N13-1#page=784" rel="nofollow">[pdf]</a></li>
<li>Large scale distributed deep networks (2012), J. Dean et al. <a href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf" rel="nofollow">[pdf]</a></li>
<li>A Fast and Accurate Dependency Parser using Neural Networks. Chen and Manning. <a href="http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf" rel="nofollow">[pdf]</a></li>
</ul>
<h2><a href="#acknowledgement" aria-hidden="true" class="anchor" id="user-content-acknowledgement"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgement</h2>
<p>Thank you for all your contributions. Please make sure to read the <a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/Contributing.md">contributing guide</a> before you make a pull request.</p>
<h2><a href="#license" aria-hidden="true" class="anchor" id="user-content-license"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>License</h2>
<p><a href="https://creativecommons.org/publicdomain/zero/1.0/" rel="nofollow"><img src="https://camo.githubusercontent.com/60561947585c982aee67ed3e3b25388184cc0aa3/687474703a2f2f6d6972726f72732e6372656174697665636f6d6d6f6e732e6f72672f70726573736b69742f627574746f6e732f38387833312f7376672f63632d7a65726f2e737667" alt="CC0" data-canonical-src="http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg" style="max-width:100%;"></a></p>
<p>To the extent possible under law, <a href="https://www.facebook.com/terryum.io/" rel="nofollow">Terry T. Um</a> has waived all copyright and related or neighboring rights to this work.</p>
</article>
  </div>


  </div>
  <div class="modal-backdrop js-touch-events"></div>
</div>

    </div>
  </div>

  </div>

      
<div class="footer container-lg px-3" role="contentinfo">
  <div class="position-relative d-flex flex-justify-between pt-6 pb-2 mt-6 f6 text-gray border-top border-gray-light ">
    <ul class="list-style-none d-flex flex-wrap ">
      <li class="mr-3">&copy; 2018 <span title="0.34544s from unicorn-3835523148-31wzf">GitHub</span>, Inc.</li>
        <li class="mr-3"><a data-ga-click="Footer, go to terms, text:terms" href="https://github.com/site/terms">Terms</a></li>
        <li class="mr-3"><a data-ga-click="Footer, go to privacy, text:privacy" href="https://github.com/site/privacy">Privacy</a></li>
        <li class="mr-3"><a href="https://help.github.com/articles/github-security/" data-ga-click="Footer, go to security, text:security">Security</a></li>
        <li class="mr-3"><a href="https://status.github.com/" data-ga-click="Footer, go to status, text:status">Status</a></li>
        <li><a data-ga-click="Footer, go to help, text:help" href="https://help.github.com">Help</a></li>
    </ul>

    <a aria-label="Homepage" title="GitHub" class="footer-octicon" href="https://github.com">
      <svg height="24" class="octicon octicon-mark-github" viewBox="0 0 16 16" version="1.1" width="24" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"/></svg>
</a>
   <ul class="list-style-none d-flex flex-wrap ">
        <li class="mr-3"><a data-ga-click="Footer, go to contact, text:contact" href="https://github.com/contact">Contact GitHub</a></li>
      <li class="mr-3"><a href="https://developer.github.com" data-ga-click="Footer, go to api, text:api">API</a></li>
      <li class="mr-3"><a href="https://training.github.com" data-ga-click="Footer, go to training, text:training">Training</a></li>
      <li class="mr-3"><a href="https://shop.github.com" data-ga-click="Footer, go to shop, text:shop">Shop</a></li>
        <li class="mr-3"><a href="https://blog.github.com" data-ga-click="Footer, go to blog, text:blog">Blog</a></li>
        <li><a data-ga-click="Footer, go to about, text:about" href="https://github.com/about">About</a></li>

    </ul>
  </div>
  <div class="d-flex flex-justify-center pb-6">
    <span class="f6 text-gray-light"></span>
  </div>
</div>



  <div id="ajax-error-message" class="ajax-error-message flash flash-error">
    <svg class="octicon octicon-alert" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.865 1.52c-.18-.31-.51-.5-.87-.5s-.69.19-.87.5L.275 13.5c-.18.31-.18.69 0 1 .19.31.52.5.87.5h13.7c.36 0 .69-.19.86-.5.17-.31.18-.69.01-1L8.865 1.52zM8.995 13h-2v-2h2v2zm0-3h-2V6h2v4z"/></svg>
    <button type="button" class="flash-close js-ajax-error-dismiss" aria-label="Dismiss error">
      <svg class="octicon octicon-x" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.48 8l3.75 3.75-1.48 1.48L6 9.48l-3.75 3.75-1.48-1.48L4.52 8 .77 4.25l1.48-1.48L6 6.52l3.75-3.75 1.48 1.48z"/></svg>
    </button>
    You can't perform that action at this time.
  </div>


    <script crossorigin="anonymous" type="application/javascript" src="https://assets-cdn.github.com/assets/compat-413dd2a0695c3dfaf7de158468a91646.js"></script>
    <script crossorigin="anonymous" type="application/javascript" src="https://assets-cdn.github.com/assets/frameworks-d941de838fad400fb91238d23684a777.js"></script>
    
    <script crossorigin="anonymous" async="async" type="application/javascript" src="https://assets-cdn.github.com/assets/github-d19623a69cc756b5a2cbda89154a69e9.js"></script>
    
    
    
    
  <div class="js-stale-session-flash stale-session-flash flash flash-warn flash-banner d-none">
    <svg class="octicon octicon-alert" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.865 1.52c-.18-.31-.51-.5-.87-.5s-.69.19-.87.5L.275 13.5c-.18.31-.18.69 0 1 .19.31.52.5.87.5h13.7c.36 0 .69-.19.86-.5.17-.31.18-.69.01-1L8.865 1.52zM8.995 13h-2v-2h2v2zm0-3h-2V6h2v4z"/></svg>
    <span class="signed-in-tab-flash">You signed in with another tab or window. <a href="">Reload</a> to refresh your session.</span>
    <span class="signed-out-tab-flash">You signed out in another tab or window. <a href="">Reload</a> to refresh your session.</span>
  </div>
  <div class="facebox" id="facebox" style="display:none;">
  <div class="facebox-popup">
    <div class="facebox-content" role="dialog" aria-labelledby="facebox-header" aria-describedby="facebox-description">
    </div>
    <button type="button" class="facebox-close js-facebox-close" aria-label="Close modal">
      <svg class="octicon octicon-x" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.48 8l3.75 3.75-1.48 1.48L6 9.48l-3.75 3.75-1.48-1.48L4.52 8 .77 4.25l1.48-1.48L6 6.52l3.75-3.75 1.48 1.48z"/></svg>
    </button>
  </div>
</div>

  <div class="Popover js-hovercard-content position-absolute" style="display: none; outline: none;" tabindex="0">
  <div class="Popover-message Popover-message--bottom-left Popover-message--large Box box-shadow-large" style="width:360px;">
  </div>
</div>

<div id="hovercard-aria-description" class="sr-only">
  Press h to open a hovercard with more details.
</div>


  </body>
</html>

